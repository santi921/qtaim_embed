{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from qtaim_embed.models.utils import (\\n    _split_batched_output,\\n    get_layer_args,\\n    link_fmt_to_node_fmt,\\n)\\n\\nfrom qtaim_embed.models.layers import (\\n    GraphConvDropoutBatch,\\n    ResidualBlock,\\n    UnifySize,\\n    Set2SetThenCat,\\n    SumPoolingThenCat,\\n    WeightAndSumThenCat,\\n    GlobalAttentionPoolingThenCat,\\n)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline GNN model for node-level regression\n",
    "# from copy import deepcopy\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.optim import lr_scheduler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from qtaim_embed.models.graph_level.base_gcn import GCNGraphPred\n",
    "from qtaim_embed.utils.data import get_default_graph_level_config\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "# import dgl.nn.pytorch as dglnn\n",
    "# from torchmetrics.wrappers import MultioutputWrapper\n",
    "# import torchmetrics\n",
    "\n",
    "\"\"\"from qtaim_embed.models.utils import (\n",
    "    _split_batched_output,\n",
    "    get_layer_args,\n",
    "    link_fmt_to_node_fmt,\n",
    ")\n",
    "\n",
    "from qtaim_embed.models.layers import (\n",
    "    GraphConvDropoutBatch,\n",
    "    ResidualBlock,\n",
    "    UnifySize,\n",
    "    Set2SetThenCat,\n",
    "    SumPoolingThenCat,\n",
    "    WeightAndSumThenCat,\n",
    "    GlobalAttentionPoolingThenCat,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef link_fmt_to_node_fmt(dict_feats):\\n    ret_dict = {}\\n    for k, v in dict_feats.items():\\n        assert k[-1] in [\"g\", \"b\", \"a\"], \"key must end with g, b, or a\"\\n        if k[-1] == \"g\":\\n            ret_dict[\"global\"] = v\\n        elif k[-1] == \"b\":\\n            ret_dict[\"bond\"] = v\\n        elif k[-1] == \"a\":\\n            ret_dict[\"atom\"] = v\\n\\n    return ret_dict\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def link_fmt_to_node_fmt(dict_feats):\n",
    "    ret_dict = {}\n",
    "    for k, v in dict_feats.items():\n",
    "        assert k[-1] in [\"g\", \"b\", \"a\"], \"key must end with g, b, or a\"\n",
    "        if k[-1] == \"g\":\n",
    "            ret_dict[\"global\"] = v\n",
    "        elif k[-1] == \"b\":\n",
    "            ret_dict[\"bond\"] = v\n",
    "        elif k[-1] == \"a\":\n",
    "            ret_dict[\"atom\"] = v\n",
    "\n",
    "    return ret_dict\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class GCNGraphPred(pl.LightningModule):\n",
    "\"\"\"\n",
    "Basic GNN model for graph-level regression\n",
    "Takes\n",
    "atom_input_size: int, dimension of atom features\n",
    "bond_input_size: int, dimension of bond features\n",
    "global_input_size: int, dimension of global features\n",
    "target_dict: dict, dictionary of targets\n",
    "n_conv_layers: int, number of convolution layers\n",
    "conv_fn: str \"GraphConvDropoutBatch\"\n",
    "dropout: float, dropout rate\n",
    "batch_norm: bool, whether to use batch norm\n",
    "activation: str, activation function\n",
    "bias: bool, whether to use bias\n",
    "norm: str, normalization type\n",
    "aggregate: str, aggregation type\n",
    "lr: float, learning rate\n",
    "scheduler_name: str, scheduler type\n",
    "weight_decay: float, weight decay\n",
    "lr_plateau_patience: int, patience for lr scheduler\n",
    "lr_scale_factor: float, scale factor for lr scheduler\n",
    "loss_fn: str, loss function\n",
    "resid_n_graph_convs: int, number of graph convolutions per residual block\n",
    "scalers: list, list of scalers applied to each node type\n",
    "embedding_size: int, size of embedding layer\n",
    "global_pooling: str, type of global pooling\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        atom_input_size=12,\n",
    "        bond_input_size=8,\n",
    "        global_input_size=3,\n",
    "        n_conv_layers=3,\n",
    "        target_dict={\"atom\": \"E\"},\n",
    "        conv_fn=\"GraphConvDropoutBatch\",\n",
    "        global_pooling=\"WeightAndSumThenCat\",\n",
    "        resid_n_graph_convs=None,\n",
    "        dropout=0.2,\n",
    "        batch_norm=True,\n",
    "        activation=\"ReLU\",\n",
    "        bias=True,\n",
    "        norm=\"both\",\n",
    "        aggregate=\"sum\",\n",
    "        lr=1e-3,\n",
    "        scheduler_name=\"reduce_on_plateau\",\n",
    "        weight_decay=0.0,\n",
    "        lr_plateau_patience=5,\n",
    "        lr_scale_factor=0.5,\n",
    "        loss_fn=\"mse\",\n",
    "        embedding_size=128,\n",
    "        fc_layer_size=[128, 64],\n",
    "        fc_dropout=0.0,\n",
    "        fc_batch_norm=True,\n",
    "        lstm_iters=3,\n",
    "        lstm_layers=1,\n",
    "        output_dims=1,\n",
    "        pooling_ntypes=[\"atom\", \"bond\"],\n",
    "        pooling_ntypes_direct=[\"global\"],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        # output_dims = 0\n",
    "        # for k, v in target_dict.items():\n",
    "        #    output_dims += len(v)\n",
    "\n",
    "        assert conv_fn == \"GraphConvDropoutBatch\" or conv_fn == \"ResidualBlock\", (\n",
    "            \"conv_fn must be either GraphConvDropoutBatch or ResidualBlock\"\n",
    "            + f\"but got {conv_fn}\"\n",
    "        )\n",
    "\n",
    "        if conv_fn == \"ResidualBlock\":\n",
    "            assert resid_n_graph_convs is not None, (\n",
    "                \"resid_n_graph_convs must be specified for ResidualBlock\"\n",
    "                + f\"but got {resid_n_graph_convs}\"\n",
    "            )\n",
    "\n",
    "        assert global_pooling in [\n",
    "            \"WeightAndSumThenCat\",\n",
    "            \"SumPoolingThenCat\",\n",
    "            \"GlobalAttentionPoolingThenCat\",\n",
    "            \"Set2SetThenCat\",\n",
    "        ], (\n",
    "            \"global_pooling must be either WeightAndSumThenCat, SumPoolingThenCat, or GlobalAttentionPoolingThenCat\"\n",
    "            + f\"but got {global_pooling}\"\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            \"atom_input_size\": atom_input_size,\n",
    "            \"bond_input_size\": bond_input_size,\n",
    "            \"global_input_size\": global_input_size,\n",
    "            \"conv_fn\": conv_fn,\n",
    "            \"target_dict\": target_dict,\n",
    "            \"output_dims\": output_dims,\n",
    "            \"dropout\": dropout,\n",
    "            \"batch_norm_tf\": batch_norm,\n",
    "            \"activation\": activation,\n",
    "            \"bias\": bias,\n",
    "            \"norm\": norm,\n",
    "            \"aggregate\": aggregate,\n",
    "            \"n_conv_layers\": n_conv_layers,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr_plateau_patience\": lr_plateau_patience,\n",
    "            \"lr_scale_factor\": lr_scale_factor,\n",
    "            \"scheduler_name\": scheduler_name,\n",
    "            \"loss_fn\": loss_fn,\n",
    "            \"resid_n_graph_convs\": resid_n_graph_convs,\n",
    "            \"embedding_size\": embedding_size,\n",
    "            \"fc_layer_size\": fc_layer_size,\n",
    "            \"fc_dropout\": fc_dropout,\n",
    "            \"fc_batch_norm\": fc_batch_norm,\n",
    "            \"n_fc_layers\": len(fc_layer_size),\n",
    "            \"global_pooling\": global_pooling,\n",
    "            \"ntypes_pool\": pooling_ntypes,\n",
    "            \"ntypes_pool_direct_cat\": pooling_ntypes_direct,\n",
    "            \"output_dims\": output_dims,\n",
    "            \"lstm_iters\": lstm_iters,\n",
    "            \"lstm_layers\": lstm_layers,\n",
    "        }\n",
    "\n",
    "        self.hparams.update(params)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # convert string activation to function\n",
    "        if self.hparams.activation is not None:\n",
    "            self.hparams.activation = getattr(torch.nn, self.hparams.activation)()\n",
    "\n",
    "        input_size = {\n",
    "            \"atom\": self.hparams.atom_input_size,\n",
    "            \"bond\": self.hparams.bond_input_size,\n",
    "            \"global\": self.hparams.global_input_size,\n",
    "        }\n",
    "        # print(\"input size\", input_size)\n",
    "        self.embedding = UnifySize(\n",
    "            input_dim=input_size,\n",
    "            output_dim=self.hparams.embedding_size,\n",
    "        )\n",
    "        # self.embedding_output_size = self.hparams.embedding_size\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        if self.hparams.conv_fn == \"GraphConvDropoutBatch\":\n",
    "            for i in range(self.hparams.n_conv_layers):\n",
    "                embedding_in = False\n",
    "                if i == 0:\n",
    "                    embedding_in = True\n",
    "\n",
    "                layer_args = get_layer_args(self.hparams, i, embedding_in=embedding_in)\n",
    "\n",
    "                self.conv_layers.append(\n",
    "                    dglnn.HeteroGraphConv(\n",
    "                        {\n",
    "                            \"a2b\": GraphConvDropoutBatch(**layer_args[\"a2b\"]),\n",
    "                            \"b2a\": GraphConvDropoutBatch(**layer_args[\"b2a\"]),\n",
    "                            \"a2g\": GraphConvDropoutBatch(**layer_args[\"a2g\"]),\n",
    "                            \"g2a\": GraphConvDropoutBatch(**layer_args[\"g2a\"]),\n",
    "                            \"b2g\": GraphConvDropoutBatch(**layer_args[\"b2g\"]),\n",
    "                            \"g2b\": GraphConvDropoutBatch(**layer_args[\"g2b\"]),\n",
    "                            \"a2a\": GraphConvDropoutBatch(**layer_args[\"a2a\"]),\n",
    "                            \"b2b\": GraphConvDropoutBatch(**layer_args[\"b2b\"]),\n",
    "                            \"g2g\": GraphConvDropoutBatch(**layer_args[\"g2g\"]),\n",
    "                        },\n",
    "                        aggregate=self.hparams.aggregate,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        elif self.hparams.conv_fn == \"ResidualBlock\":\n",
    "            layer_tracker = 0\n",
    "            embedding_in = False\n",
    "            if layer_tracker == 0:\n",
    "                embedding_in = True\n",
    "\n",
    "            while layer_tracker < self.hparams.n_conv_layers:\n",
    "                if (\n",
    "                    layer_tracker + self.hparams.resid_n_graph_convs\n",
    "                    > self.hparams.n_conv_layers - 1\n",
    "                ):\n",
    "                    # print(\"triggered output_layer args\")\n",
    "                    layer_ind = self.hparams.n_conv_layers - layer_tracker - 1\n",
    "                else:\n",
    "                    layer_ind = -1\n",
    "\n",
    "                layer_args = get_layer_args(\n",
    "                    self.hparams, layer_ind, embedding_in=embedding_in\n",
    "                )\n",
    "\n",
    "                output_block = False\n",
    "                if layer_ind != -1:\n",
    "                    output_block = True\n",
    "\n",
    "                self.conv_layers.append(\n",
    "                    ResidualBlock(\n",
    "                        layer_args,\n",
    "                        resid_n_graph_convs=self.hparams.resid_n_graph_convs,\n",
    "                        aggregate=self.hparams.aggregate,\n",
    "                        output_block=output_block,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                layer_tracker += self.hparams.resid_n_graph_convs\n",
    "\n",
    "        self.conv_layers = nn.ModuleList(self.conv_layers)\n",
    "        # print(\"conv layer out modes\", self.conv_layers[-1].mods)\n",
    "\n",
    "        # print(\"conv layer out feats\", self.conv_layers[-1].out_feats)\n",
    "        # conv_out_size = self.conv_layers[-1].out_feats\n",
    "\n",
    "        if self.hparams.conv_fn == \"GraphConvDropoutBatch\":\n",
    "            conv_out_size = {}\n",
    "            for k, v in self.conv_layers[-1].mods.items():\n",
    "                conv_out_size[k] = v.out_feats\n",
    "        elif self.hparams.conv_fn == \"ResidualBlock\":\n",
    "            conv_out_size = self.conv_layers[-1].out_feats\n",
    "\n",
    "        # print(\"conv out raw\", conv_out_size)\n",
    "        self.conv_out_size = link_fmt_to_node_fmt(conv_out_size)\n",
    "        # print(\"conv out size: \", self.conv_out_size)\n",
    "\n",
    "        if self.hparams.global_pooling == \"WeightAndSumThenCat\":\n",
    "            readout_fn = WeightAndSumThenCat\n",
    "        elif self.hparams.global_pooling == \"SumPoolingThenCat\":\n",
    "            readout_fn = SumPoolingThenCat\n",
    "        elif self.hparams.global_pooling == \"GlobalAttentionPoolingThenCat\":\n",
    "            readout_fn = GlobalAttentionPoolingThenCat\n",
    "        elif self.hparams.global_pooling == \"Set2SetThenCat\":\n",
    "            readout_fn = Set2SetThenCat\n",
    "\n",
    "        list_in_feats = []\n",
    "        for type_feat in self.hparams.pooling_ntypes:\n",
    "            list_in_feats.append(self.conv_out_size[type_feat])\n",
    "\n",
    "        self.readout_out_size = 0\n",
    "        # print(\"list in feats\", list_in_feats)\n",
    "        # print(\"conv out size\", self.conv_out_size)\n",
    "        # print(\"pooling ntypes\", self.hparams.pooling_ntypes)\n",
    "        # print(\"pooling ntypes direct cat\", self.hparams.ntypes_pool_direct_cat)\n",
    "        if self.hparams.global_pooling == \"Set2SetThenCat\":\n",
    "            # print(\"using set2setthencat\")\n",
    "\n",
    "            self.readout = readout_fn(\n",
    "                n_iters=self.hparams.lstm_iters,\n",
    "                n_layers=self.hparams.lstm_layers,\n",
    "                in_feats=list_in_feats,\n",
    "                ntypes=self.hparams.pooling_ntypes,\n",
    "                ntypes_direct_cat=self.hparams.ntypes_pool_direct_cat,\n",
    "            )\n",
    "            for i in self.hparams.pooling_ntypes:\n",
    "                if i not in self.hparams.ntypes_pool_direct_cat:\n",
    "                    self.readout_out_size += self.conv_out_size[i] * 2\n",
    "                else:\n",
    "                    self.readout_out_size += self.conv_out_size[i]\n",
    "\n",
    "        else:\n",
    "            # print(\"other readout used\")\n",
    "            self.readout = readout_fn(\n",
    "                ntypes=self.hparams.pooling_ntypes,\n",
    "                in_feats=list_in_feats,\n",
    "                ntypes_direct_cat=self.hparams.ntypes_pool_direct_cat,\n",
    "            )\n",
    "\n",
    "            for i in self.hparams.pooling_ntypes:\n",
    "                if i in self.hparams.ntypes_pool_direct_cat:\n",
    "                    self.readout_out_size += self.conv_out_size[i]\n",
    "                else:\n",
    "                    self.readout_out_size += self.conv_out_size[i]\n",
    "\n",
    "        # print(\"readout out size\", self.readout_out_size)\n",
    "        # self.readout_out_size = readout_out_size\n",
    "        self.loss = self.loss_function()\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        input_size = self.readout_out_size\n",
    "        for i in range(self.hparams.n_fc_layers):\n",
    "            out_size = self.hparams.fc_layer_size[i]\n",
    "            self.fc_layers.append(nn.Linear(input_size, out_size))\n",
    "            if self.hparams.fc_batch_norm:\n",
    "                self.fc_layers.append(nn.BatchNorm1d(out_size))\n",
    "            if self.hparams.activation is not None:\n",
    "                self.fc_layers.append(self.hparams.activation)\n",
    "            if self.hparams.fc_dropout > 0:\n",
    "                self.fc_layers.append(nn.Dropout(self.hparams.fc_dropout))\n",
    "            input_size = out_size\n",
    "\n",
    "        self.fc_layers.append(nn.Linear(input_size, self.hparams.output_dims))\n",
    "\n",
    "        # print(\"number of output dims\", output_dims)\n",
    "\n",
    "        # create multioutput wrapper for metrics\n",
    "        self.train_r2 = MultioutputWrapper(\n",
    "            torchmetrics.R2Score(), num_outputs=self.hparams.output_dims\n",
    "        )\n",
    "        self.train_torch_l1 = MultioutputWrapper(\n",
    "            torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.output_dims\n",
    "        )\n",
    "        self.train_torch_mse = MultioutputWrapper(\n",
    "            torchmetrics.MeanSquaredError(squared=False),\n",
    "            num_outputs=self.hparams.output_dims,\n",
    "        )\n",
    "        self.val_r2 = MultioutputWrapper(\n",
    "            torchmetrics.R2Score(), num_outputs=self.hparams.output_dims\n",
    "        )\n",
    "        self.val_torch_l1 = MultioutputWrapper(\n",
    "            torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.output_dims\n",
    "        )\n",
    "        self.val_torch_mse = MultioutputWrapper(\n",
    "            torchmetrics.MeanSquaredError(squared=False),\n",
    "            num_outputs=self.hparams.output_dims,\n",
    "        )\n",
    "        self.test_r2 = MultioutputWrapper(\n",
    "            torchmetrics.R2Score(), num_outputs=self.hparams.output_dims\n",
    "        )\n",
    "        self.test_torch_l1 = MultioutputWrapper(\n",
    "            torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.output_dims\n",
    "        )\n",
    "        self.test_torch_mse = MultioutputWrapper(\n",
    "            torchmetrics.MeanSquaredError(squared=False),\n",
    "            num_outputs=self.hparams.output_dims,\n",
    "        )\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        feats = self.embedding(inputs)\n",
    "        for ind, conv in enumerate(self.conv_layers):\n",
    "            feats = conv(graph, feats)\n",
    "\n",
    "        readout_feats = self.readout(graph, feats)\n",
    "        for ind, layer in enumerate(self.fc_layers):\n",
    "            readout_feats = layer(readout_feats)\n",
    "\n",
    "        # print(\"preds shape:\", readout_feats.shape)\n",
    "        return readout_feats\n",
    "\n",
    "    def loss_function(self):\n",
    "        \"\"\"\n",
    "        Initialize loss function\n",
    "        \"\"\"\n",
    "        if self.hparams.loss_fn == \"mse\":\n",
    "            # make multioutput wrapper for mse\n",
    "            loss_multi = MultioutputWrapper(\n",
    "                torchmetrics.MeanSquaredError(), num_outputs=self.hparams.output_dims\n",
    "            )\n",
    "        elif self.hparams.loss_fn == \"smape\":\n",
    "            loss_multi = MultioutputWrapper(\n",
    "                torchmetrics.SymmetricMeanAbsolutePercentageError(),\n",
    "                num_outputs=self.hparams.output_dims,\n",
    "            )\n",
    "        elif self.hparams.loss_fn == \"mae\":\n",
    "            loss_multi = MultioutputWrapper(\n",
    "                torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.output_dims\n",
    "            )\n",
    "        else:\n",
    "            loss_multi = MultioutputWrapper(\n",
    "                torchmetrics.MeanSquaredError(), num_outputs=self.hparams.output_dims\n",
    "            )\n",
    "\n",
    "        loss_fn = loss_multi\n",
    "        return loss_fn\n",
    "\n",
    "    def compute_loss(self, target, pred):\n",
    "        \"\"\"\n",
    "        Compute loss\n",
    "        \"\"\"\n",
    "        return self.loss(target, pred)\n",
    "\n",
    "    def feature_at_each_layer(model, graph, feats):\n",
    "        \"\"\"\n",
    "        Get the features at each layer before the final fully-connected layer.\n",
    "\n",
    "        This is used for feature visualization to see how the model learns.\n",
    "\n",
    "        Returns:\n",
    "            dict: (layer_idx, feats), each feats is a list of\n",
    "        \"\"\"\n",
    "\n",
    "        layer_idx = 0\n",
    "        atom_feats, bond_feats, global_feats = {}, {}, {}\n",
    "\n",
    "        feats = model.embedding(feats)\n",
    "        bond_feats[layer_idx] = _split_batched_output(graph, feats[\"bond\"], \"bond\")\n",
    "        atom_feats[layer_idx] = _split_batched_output(graph, feats[\"atom\"], \"atom\")\n",
    "        global_feats[layer_idx] = _split_batched_output(\n",
    "            graph, feats[\"global\"], \"global\"\n",
    "        )\n",
    "\n",
    "        layer_idx += 1\n",
    "\n",
    "        # gated layer\n",
    "        for layer in model.conv_layers[:-1]:\n",
    "            feats = layer(graph, feats)\n",
    "            # store bond feature of each molecule\n",
    "            bond_feats[layer_idx] = _split_batched_output(graph, feats[\"bond\"], \"bond\")\n",
    "\n",
    "            atom_feats[layer_idx] = _split_batched_output(graph, feats[\"atom\"], \"atom\")\n",
    "\n",
    "            global_feats[layer_idx] = _split_batched_output(\n",
    "                graph, feats[\"global\"], \"global\"\n",
    "            )\n",
    "            layer_idx += 1\n",
    "\n",
    "    def shared_step(self, batch, mode):\n",
    "        batch_graph, batch_label = batch\n",
    "        logits = self.forward(\n",
    "            batch_graph, batch_graph.ndata[\"feat\"]\n",
    "        )  # returns a dict of node types\n",
    "        # max_nodes = -1\n",
    "        # for target_type, target_list in self.hparams.target_dict.items():\n",
    "        #    if target_list is not None and len(target_list) > 0:\n",
    "        #        labels = batch_label[target_type]\n",
    "        #        logits_temp = logits[target_type]\n",
    "        #        if max_nodes < logits_temp.shape[0]:\n",
    "        #            max_nodes = logits_temp.shape[0]\n",
    "        #        logits_list.append(logits_temp)\n",
    "        #        labels_list.append(labels)\n",
    "        labels = batch_label[\"global\"]\n",
    "        labels\n",
    "\n",
    "        # logits_list = [F.pad(i, (0, 0, 0, max_nodes - i.shape[0])) for i in logits_list]\n",
    "        # labels_list = [F.pad(i, (0, 0, 0, max_nodes - i.shape[0])) for i in labels_list]\n",
    "        # logits = torch.cat(logits, dim=1)\n",
    "        # labels = torch.cat(labels, dim=1)\n",
    "\n",
    "        all_loss = self.compute_loss(logits, labels)\n",
    "\n",
    "        # log loss\n",
    "        self.log(\n",
    "            f\"{mode}_loss\",\n",
    "            all_loss.sum(),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            batch_size=len(labels),\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.update_metrics(logits, labels, mode)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Train step\n",
    "        \"\"\"\n",
    "        return self.shared_step(batch, mode=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Val step\n",
    "        \"\"\"\n",
    "        return self.shared_step(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Todo\n",
    "        return self.shared_step(batch, mode=\"test\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Training epoch end\n",
    "        \"\"\"\n",
    "        r2, mae, mse = self.compute_metrics(mode=\"train\")\n",
    "        # get epoch number\n",
    "        if self.trainer.current_epoch == 0:\n",
    "            self.log(\"val_mae\", 10**10, prog_bar=False)\n",
    "        self.log(\"train_r2\", r2.median(), prog_bar=False, sync_dist=True)\n",
    "        self.log(\"train_mae\", mae.mean(), prog_bar=False, sync_dist=True)\n",
    "        self.log(\"train_mse\", mse.mean(), prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Validation epoch end\n",
    "        \"\"\"\n",
    "        r2, mae, mse = self.compute_metrics(mode=\"val\")\n",
    "        r2_median = r2.median().type(torch.float32)\n",
    "        self.log(\"val_r2\", r2_median, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_mae\", mae.mean(), prog_bar=False, sync_dist=True)\n",
    "        self.log(\"val_mse\", mse.mean(), prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Test epoch end\n",
    "        \"\"\"\n",
    "        r2, mae, mse = self.compute_metrics(mode=\"test\")\n",
    "        self.log(\"test_r2\", r2.median(), prog_bar=False, sync_dist=True)\n",
    "        self.log(\"test_mae\", mae.mean(), prog_bar=False, sync_dist=True)\n",
    "        self.log(\"test_mse\", mse.mean(), prog_bar=False, sync_dist=True)\n",
    "\n",
    "    def update_metrics(self, pred, target, mode):\n",
    "        \"\"\"\n",
    "        Update metrics using torchmetrics interfaces\n",
    "        \"\"\"\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.train_r2.update(pred, target)\n",
    "            self.train_torch_l1.update(pred, target)\n",
    "            self.train_torch_mse.update(pred, target)\n",
    "        elif mode == \"val\":\n",
    "            self.val_r2.update(pred, target)\n",
    "            self.val_torch_l1.update(pred, target)\n",
    "            self.val_torch_mse.update(pred, target)\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            self.test_r2.update(pred, target)\n",
    "            self.test_torch_l1.update(pred, target)\n",
    "            self.test_torch_mse.update(pred, target)\n",
    "\n",
    "    def compute_metrics(self, mode):\n",
    "        \"\"\"\n",
    "        Compute metrics using torchmetrics interfaces\n",
    "        \"\"\"\n",
    "\n",
    "        if mode == \"train\":\n",
    "            r2 = self.train_r2.compute()\n",
    "            torch_l1 = self.train_torch_l1.compute()\n",
    "            torch_mse = self.train_torch_mse.compute()\n",
    "            self.train_r2.reset()\n",
    "            self.train_torch_l1.reset()\n",
    "            self.train_torch_mse.reset()\n",
    "\n",
    "        elif mode == \"val\":\n",
    "            r2 = self.val_r2.compute()\n",
    "            torch_l1 = self.val_torch_l1.compute()\n",
    "            torch_mse = self.val_torch_mse.compute()\n",
    "            self.val_r2.reset()\n",
    "            self.val_torch_l1.reset()\n",
    "            self.val_torch_mse.reset()\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            r2 = self.test_r2.compute()\n",
    "            torch_l1 = self.test_torch_l1.compute()\n",
    "            torch_mse = self.test_torch_mse.compute()\n",
    "            self.test_r2.reset()\n",
    "            self.test_torch_l1.reset()\n",
    "            self.test_torch_mse.reset()\n",
    "\n",
    "        return r2, torch_l1, torch_mse\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "\n",
    "        scheduler = self._config_lr_scheduler(optimizer)\n",
    "\n",
    "        lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_mae\"}\n",
    "\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _config_lr_scheduler(self, optimizer):\n",
    "        scheduler_name = self.hparams[\"scheduler_name\"].lower()\n",
    "\n",
    "        if scheduler_name == \"reduce_on_plateau\":\n",
    "            scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode=\"max\",\n",
    "                factor=self.hparams.lr_scale_factor,\n",
    "                patience=self.hparams.lr_plateau_patience,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        elif scheduler_name == \"none\":\n",
    "            scheduler = None\n",
    "        else:\n",
    "            raise ValueError(f\"Not supported lr scheduler: {scheduler_name}\")\n",
    "\n",
    "        return scheduler\n",
    "\n",
    "    def evaluate_manually(self, batch_graph, batched_label, scaler_list):\n",
    "        \"\"\"\n",
    "        Evaluate a set of data manually\n",
    "        Takes\n",
    "            feats: dict, dictionary of batched features\n",
    "            scaler_list: list, list of scalers\n",
    "        \"\"\"\n",
    "        # batch_graph, batch_label = batch\n",
    "        preds = self.forward(batch_graph, batched_label)\n",
    "        preds_unscaled = deepcopy(preds)\n",
    "        labels_unscaled = deepcopy(batched_label)\n",
    "        for scaler in scaler_list:\n",
    "            labels_unscaled = scaler.inverse_feats(labels_unscaled)\n",
    "            preds_unscaled = scaler.inverse_feats(preds_unscaled)\n",
    "\n",
    "        # manually compute metrics\n",
    "        r2 = torchmetrics.R2Score()\n",
    "        mae = torchmetrics.MeanAbsoluteError()\n",
    "        mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "        r2.update(preds_unscaled, labels_unscaled)\n",
    "        mae.update(preds_unscaled, labels_unscaled)\n",
    "        mse.update(preds_unscaled, labels_unscaled)\n",
    "\n",
    "        r2 = r2.compute()\n",
    "        mae = mae.compute()\n",
    "        mse = mse.compute()\n",
    "\n",
    "        return r2, mae, mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from qtaim_embed.core.dataset import (\\n    HeteroGraphNodeLabelDataset,\\n    Subset,\\n    HeteroGraphGraphLabelDataset,\\n)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from qtaim_embed.utils.grapher import get_grapher\n",
    "# from qtaim_embed.data.molwrapper import mol_wrappers_from_df\n",
    "\n",
    "\"\"\"from qtaim_embed.core.dataset import (\n",
    "    HeteroGraphNodeLabelDataset,\n",
    "    Subset,\n",
    "    HeteroGraphGraphLabelDataset,\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_default_graph_level_config()\n",
    "config[\"log_scale_features\"] = True\n",
    "config[\"log_scale_targets\"] = False\n",
    "config[\"standard_scale_features\"] = True\n",
    "config[\"standard_scale_targets\"] = True\n",
    "config[\"debug\"] = False\n",
    "config[\n",
    "    \"train_dataset_loc\"\n",
    "] = \"/home/santiagovargas/dev/qtaim_embed/data/xyz_qm8/molecules_qtaim_labelled.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_dataset = HeteroGraphGraphLabelDataset(\\n    file=config[\"train_dataset_loc\"],\\n    allowed_ring_size=config[\"allowed_ring_size\"],\\n    allowed_charges=config[\"allowed_charges\"],\\n    self_loop=True,\\n    extra_keys=config[\"extra_keys\"],\\n    target_list=config[\"target_list\"],\\n    extra_dataset_info=config[\"extra_dataset_info\"],\\n    debug=config[\"debug\"],\\n    standard_scale_features=config[\"standard_scale_features\"],\\n    standard_scale_targets=config[\"standard_scale_targets\"],\\n    log_scale_features=config[\"log_scale_features\"],\\n    log_scale_targets=config[\"log_scale_targets\"],\\n)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_dataset = HeteroGraphGraphLabelDataset(\n",
    "    file=config[\"train_dataset_loc\"],\n",
    "    allowed_ring_size=config[\"allowed_ring_size\"],\n",
    "    allowed_charges=config[\"allowed_charges\"],\n",
    "    self_loop=True,\n",
    "    extra_keys=config[\"extra_keys\"],\n",
    "    target_list=config[\"target_list\"],\n",
    "    extra_dataset_info=config[\"extra_dataset_info\"],\n",
    "    debug=config[\"debug\"],\n",
    "    standard_scale_features=config[\"standard_scale_features\"],\n",
    "    standard_scale_targets=config[\"standard_scale_targets\"],\n",
    "    log_scale_features=config[\"log_scale_features\"],\n",
    "    log_scale_targets=config[\"log_scale_targets\"],\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom qtaim_embed.data.dataloader import DataLoaderMoleculeGraphTask\\n\\ndataloader = DataLoaderMoleculeGraphTask(train_dataset, batch_size=200, shuffle=True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from qtaim_embed.data.dataloader import DataLoaderMoleculeGraphTask\n",
    "\n",
    "dataloader = DataLoaderMoleculeGraphTask(train_dataset, batch_size=200, shuffle=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbatch_graph, batch_label = next(iter(dataloader))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "batch_graph, batch_label = next(iter(dataloader))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feat_size = train_dataset.feature_size()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"feat_size = train_dataset.feature_size()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# \"GraphConvDropoutBatch\",\\nfor function in [\\n    \"GlobalAttentionPoolingThenCat\",\\n    \"Set2SetThenCat\",\\n    \"WeightAndSumThenCat\",\\n    \"SumPoolingThenCat\",\\n]:\\n    for conv_fn in [\"ResidualBlock\", \"GraphConvDropoutBatch\"]:\\n        print(f\"function: {function}, conv_fn: {conv_fn}\")\\n        model = GCNGraphPred(\\n            atom_input_size=feat_size[\"atom\"],\\n            bond_input_size=feat_size[\"bond\"],\\n            global_input_size=feat_size[\"global\"],\\n            n_conv_layers=5,\\n            resid_n_graph_convs=2,\\n            target_dict={\"global\": \"extra_feat_global_E1_CAM\"},\\n            conv_fn=conv_fn,\\n            global_pooling=function,\\n            dropout=0.2,\\n            batch_norm=True,\\n            activation=None,\\n            bias=True,\\n            norm=\"both\",\\n            aggregate=\"sum\",\\n            lr=1e-2,\\n            scheduler_name=\"reduce_on_plateau\",\\n            weight_decay=0.0,\\n            lr_plateau_patience=5,\\n            lr_scale_factor=0.8,\\n            loss_fn=\"mse\",\\n            embedding_size=24,\\n            fc_layer_size=[256, 128, 128],\\n            fc_dropout=0.2,\\n            fc_batch_norm=True,\\n            lstm_iters=3,\\n            lstm_layers=1,\\n            output_dims=1,\\n            pooling_ntypes=[\"atom\", \"bond\", \"global\"],\\n            pooling_ntypes_direct=[\"global\"],\\n        )\\n    batch_graph, batch_label = next(iter(dataloader))\\n    # out = model.forward(batch_graph, batch_graph.ndata[\"feat\"])\\n    out = model.shared_step((batch_graph, batch_label), \"train\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# \"GraphConvDropoutBatch\",\n",
    "for function in [\n",
    "    \"GlobalAttentionPoolingThenCat\",\n",
    "    \"Set2SetThenCat\",\n",
    "    \"WeightAndSumThenCat\",\n",
    "    \"SumPoolingThenCat\",\n",
    "]:\n",
    "    for conv_fn in [\"ResidualBlock\", \"GraphConvDropoutBatch\"]:\n",
    "        print(f\"function: {function}, conv_fn: {conv_fn}\")\n",
    "        model = GCNGraphPred(\n",
    "            atom_input_size=feat_size[\"atom\"],\n",
    "            bond_input_size=feat_size[\"bond\"],\n",
    "            global_input_size=feat_size[\"global\"],\n",
    "            n_conv_layers=5,\n",
    "            resid_n_graph_convs=2,\n",
    "            target_dict={\"global\": \"extra_feat_global_E1_CAM\"},\n",
    "            conv_fn=conv_fn,\n",
    "            global_pooling=function,\n",
    "            dropout=0.2,\n",
    "            batch_norm=True,\n",
    "            activation=None,\n",
    "            bias=True,\n",
    "            norm=\"both\",\n",
    "            aggregate=\"sum\",\n",
    "            lr=1e-2,\n",
    "            scheduler_name=\"reduce_on_plateau\",\n",
    "            weight_decay=0.0,\n",
    "            lr_plateau_patience=5,\n",
    "            lr_scale_factor=0.8,\n",
    "            loss_fn=\"mse\",\n",
    "            embedding_size=24,\n",
    "            fc_layer_size=[256, 128, 128],\n",
    "            fc_dropout=0.2,\n",
    "            fc_batch_norm=True,\n",
    "            lstm_iters=3,\n",
    "            lstm_layers=1,\n",
    "            output_dims=1,\n",
    "            pooling_ntypes=[\"atom\", \"bond\", \"global\"],\n",
    "            pooling_ntypes_direct=[\"global\"],\n",
    "        )\n",
    "    batch_graph, batch_label = next(iter(dataloader))\n",
    "    # out = model.forward(batch_graph, batch_graph.ndata[\"feat\"])\n",
    "    out = model.shared_step((batch_graph, batch_label), \"train\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > creating MoleculeWrapper objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21786/21786 [00:02<00:00, 8861.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > bond_feats_error_count:  1\n",
      "... > atom_feats_error_count:  1\n",
      "element set {'F', 'O', 'C', 'N', 'H'}\n",
      "selected atomic keys ['extra_feat_atom_esp_total']\n",
      "selected bond keys ['extra_feat_bond_esp_total', 'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta', 'bond_length']\n",
      "selected global keys ['extra_feat_global_E1_CAM']\n",
      "... > Building graphs and featurizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21785/21785 [01:05<00:00, 333.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included in labels\n",
      "{'global': ['extra_feat_global_E1_CAM']}\n",
      "included in graph features\n",
      "{'atom': ['total_degree', 'total_H', 'is_in_ring', 'ring_size_3', 'ring_size_4', 'ring_size_5', 'ring_size_6', 'ring_size_7', 'chemical_symbol_F', 'chemical_symbol_O', 'chemical_symbol_C', 'chemical_symbol_N', 'chemical_symbol_H', 'extra_feat_atom_esp_total'], 'bond': ['metal bond', 'ring inclusion', 'ring size_3', 'ring size_4', 'ring size_5', 'ring size_6', 'ring size_7', 'bond_length', 'extra_feat_bond_esp_total', 'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta'], 'global': ['num atoms', 'num bonds', 'molecule weight']}\n",
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys([])\n",
      "include names:  dict_keys(['global'])\n",
      "... > parsing labels and features in graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21785/21785 [00:00<00:00, 34472.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys(['global'])\n",
      "... > Log scaling features\n",
      "... > Log scaling features complete\n",
      "... > Scaling features\n",
      "Standard deviation for feature 0 is 0.0, smaller than 0.001. You may want to exclude this feature.\n",
      "... > Scaling features complete\n",
      "... > feature mean(s): \n",
      " {'atom': tensor([1.0344e+00, 2.9874e-01, 1.7718e-01, 3.5022e-02, 3.6926e-02, 6.3978e-02,\n",
      "        3.5200e-02, 6.0529e-03, 1.0124e-03, 5.4512e-02, 2.3809e-01, 4.1223e-02,\n",
      "        3.5831e-01, 8.9978e+00]), 'bond': tensor([0.0000, 0.1791, 0.0362, 0.0396, 0.0663, 0.0377, 0.0070, 0.8149, 0.6613,\n",
      "        0.0727, 0.8958]), 'global': tensor([2.8232, 2.8453, 4.6960])}\n",
      "... > feature std(s):  \n",
      " {'atom': tensor([0.3963, 0.4616, 0.3024, 0.1518, 0.1557, 0.2006, 0.1522, 0.0645, 0.0265,\n",
      "        0.1866, 0.3292, 0.1639, 0.3464, 5.6739]), 'bond': tensor([0.0010, 0.3034, 0.1543, 0.1608, 0.2038, 0.1571, 0.0694, 0.0916, 0.1841,\n",
      "        0.1234, 0.1575]), 'global': tensor([0.1786, 0.1933, 0.0854])}\n",
      "... > Scaling targets\n",
      "... > Scaling targets complete\n",
      "... > feature mean(s): \n",
      " {'global': tensor([0.2168])}\n",
      "... > feature std(s):  \n",
      " {'global': tensor([0.0446])}\n",
      "... > loaded dataset\n"
     ]
    }
   ],
   "source": [
    "from qtaim_embed.core.datamodule import QTAIMGraphTaskDataModule\n",
    "\n",
    "dm = QTAIMGraphTaskDataModule(\n",
    "    config=config,\n",
    ")\n",
    "feat_size = dm.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNGraphPred(\n",
    "    atom_input_size=feat_size[\"atom\"],\n",
    "    bond_input_size=feat_size[\"bond\"],\n",
    "    global_input_size=feat_size[\"global\"],\n",
    "    n_conv_layers=3,\n",
    "    resid_n_graph_convs=2,\n",
    "    target_dict={\"global\": \"extra_feat_global_E1_CAM\"},\n",
    "    conv_fn=\"GraphConvDropoutBatch\",\n",
    "    global_pooling=\"Set2SetThenCat\",\n",
    "    dropout=0.2,\n",
    "    batch_norm=False,\n",
    "    activation=\"ReLU\",\n",
    "    bias=True,\n",
    "    norm=\"both\",\n",
    "    aggregate=\"sum\",\n",
    "    lr=0.01,\n",
    "    scheduler_name=\"reduce_on_plateau\",\n",
    "    weight_decay=0.00001,\n",
    "    lr_plateau_patience=25,\n",
    "    lr_scale_factor=0.8,\n",
    "    loss_fn=\"mae\",\n",
    "    embedding_size=10,\n",
    "    fc_layer_size=[256, 128, 128],\n",
    "    fc_dropout=0.2,\n",
    "    fc_batch_norm=True,\n",
    "    lstm_iters=3,\n",
    "    lstm_layers=2,\n",
    "    output_dims=1,\n",
    "    pooling_ntypes=[\"atom\", \"bond\", \"global\"],\n",
    "    pooling_ntypes_direct=[\"global\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanti\u001b[0m (\u001b[33mhydro_homies\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/santiagovargas/dev/qtaim_embed/qtaim_embed/scripts/notebooks/wandb/run-20231023_132004-ht6489vv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hydro_homies/qtaim_embed_test/runs/ht6489vv' target=\"_blank\">generous-star-6</a></strong> to <a href='https://wandb.ai/hydro_homies/qtaim_embed_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hydro_homies/qtaim_embed_test' target=\"_blank\">https://wandb.ai/hydro_homies/qtaim_embed_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hydro_homies/qtaim_embed_test/runs/ht6489vv' target=\"_blank\">https://wandb.ai/hydro_homies/qtaim_embed_test/runs/ht6489vv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:398: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./test_logs/test_logs\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/santiagovargas/dev/qtaim_embed/qtaim_embed/scripts/notebooks/test_logs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name            | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0  | embedding       | UnifySize          | 280   \n",
      "1  | conv_layers     | ModuleList         | 3.3 K \n",
      "2  | readout         | Set2SetThenCat     | 6.7 K \n",
      "3  | loss            | MultioutputWrapper | 0     \n",
      "4  | fc_layers       | ModuleList         | 69.8 K\n",
      "5  | train_r2        | MultioutputWrapper | 0     \n",
      "6  | train_torch_l1  | MultioutputWrapper | 0     \n",
      "7  | train_torch_mse | MultioutputWrapper | 0     \n",
      "8  | val_r2          | MultioutputWrapper | 0     \n",
      "9  | val_torch_l1    | MultioutputWrapper | 0     \n",
      "10 | val_torch_mse   | MultioutputWrapper | 0     \n",
      "11 | test_r2         | MultioutputWrapper | 0     \n",
      "12 | test_torch_l1   | MultioutputWrapper | 0     \n",
      "13 | test_torch_mse  | MultioutputWrapper | 0     \n",
      "--------------------------------------------------------\n",
      "80.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.1 K    Total params\n",
      "0.320     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055e935dc8fa4289bb1434ba7216d288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a43729741e41cd85f18711c54ccb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe733f5c4ad744a68f7ff3b94e0a71f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('val_mae', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e816b3ffea480bb696aef6e4a741ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c338a093bd44b097780204cb993895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9dfdf5b7394367bbbb4c1e53616f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9313f2ef2d407697113ee5400ef927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9999a4ffe1d8489a974dfde81d146bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f41e97bece84e90a34038fc0186447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af073ae029c34a2fa4a2976f1d9420bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef18267643a14004a33dbda481c8ae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649c5c32651548b7a8c753044ca894c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ec9c14c0a74e4f8b08cdfd6861fd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7772d7ef8242c3bb5c4192b2258bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d6b1f3a3ad4d008eca0dd2e635ae46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0fc6b34f7f4a398b4802a58b1ae39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.297951…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_mse</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_r2</td><td>▁▅▆▇▇▇▇██████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▄▅▃▂▂▂▂▁▁▂▂</td></tr><tr><td>val_mae</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▆▄▄▃▂▂▂▂▁▁▂▂</td></tr><tr><td>val_r2</td><td>▁▃▅▅▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>lr-Adam</td><td>0.01</td></tr><tr><td>train_loss</td><td>6.24149</td></tr><tr><td>train_mae</td><td>6.24149</td></tr><tr><td>train_mse</td><td>8.66831</td></tr><tr><td>train_r2</td><td>0.85057</td></tr><tr><td>trainer/global_step</td><td>1689</td></tr><tr><td>val_loss</td><td>5.39834</td></tr><tr><td>val_mae</td><td>5.39834</td></tr><tr><td>val_mse</td><td>7.62241</td></tr><tr><td>val_r2</td><td>0.88487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generous-star-6</strong> at: <a href='https://wandb.ai/hydro_homies/qtaim_embed_test/runs/ht6489vv' target=\"_blank\">https://wandb.ai/hydro_homies/qtaim_embed_test/runs/ht6489vv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231023_132004-ht6489vv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from qtaim_embed.models.utils import LogParameters\n",
    "import wandb\n",
    "\n",
    "with wandb.init(project=\"qtaim_embed_test\") as run:\n",
    "    logger_tb = TensorBoardLogger(\"./test_logs\", name=\"test_logs\")\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"test_logs\",\n",
    "        filename=\"model_lightning_{epoch:02d}-{val_mae:.2f}\",\n",
    "        monitor=\"val_mae\",\n",
    "        mode=\"min\",\n",
    "        auto_insert_metric_name=True,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_mae\",\n",
    "        min_delta=0.00,\n",
    "        patience=500,\n",
    "        verbose=False,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    logger_wb = WandbLogger(name=\"test_logs\")\n",
    "    log_parameters = LogParameters()\n",
    "    trainer_transfer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_val=3.0,\n",
    "        default_root_dir=\"./test/\",\n",
    "        precision=\"32\",\n",
    "        log_every_n_steps=10,\n",
    "        callbacks=[\n",
    "            early_stopping_callback,\n",
    "            lr_monitor,\n",
    "            log_parameters,\n",
    "            checkpoint_callback,\n",
    "        ],\n",
    "        enable_checkpointing=True,\n",
    "        logger=[logger_tb, logger_wb],\n",
    "    )\n",
    "\n",
    "    # move model to gpu\n",
    "    # model = model.cuda()\n",
    "\n",
    "    trainer_transfer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 128/128 [00:06<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-33.24173132586083 249.59429794549942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 128/128 [00:05<00:00, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975651399649531 145.10985386371613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 128/128 [00:05<00:00, 23.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6941618842251431 116.58790373802185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 128/128 [00:05<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7409113617603024 101.8263692855835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 128/128 [00:05<00:00, 23.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7836339133675434 90.58988931775093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 128/128 [00:05<00:00, 23.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7928580934080458 85.0909181535244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 128/128 [00:05<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8063137417976194 82.7260967195034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 128/128 [00:05<00:00, 23.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160909185164501 78.84068021178246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 128/128 [00:05<00:00, 24.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819367901339974 76.51211869716644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 128/128 [00:05<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8249671264120335 75.31890374422073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 128/128 [00:05<00:00, 23.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8328095992253378 72.14961007237434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 128/128 [00:05<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8268704146833308 73.46147549152374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 128/128 [00:05<00:00, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289035893325564 73.41576477885246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 128/128 [00:05<00:00, 23.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8252398722538921 74.98519903421402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 128/128 [00:05<00:00, 23.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8434109495799262 67.3860405087471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16:  98%|█████████▊| 126/128 [00:05<00:00, 23.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m training_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m step, (batch_graph, batch_label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tq):\n\u001b[1;32m     24\u001b[0m     \u001b[39m# forward propagation by using all nodes and extracting the user embeddings\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     batch_graph, batch_label \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloader))\n\u001b[1;32m     26\u001b[0m     labels \u001b[39m=\u001b[39m batch_label[\u001b[39m\"\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(batch_graph, batch_graph\u001b[39m.\u001b[39mndata[\u001b[39m\"\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/dev/qtaim_embed/qtaim_embed/data/dataloader.py:43\u001b[0m, in \u001b[0;36mDataLoaderMoleculeGraphTask.__init__.<locals>.collate\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollate\u001b[39m(samples):\n\u001b[1;32m     42\u001b[0m     graphs \u001b[39m=\u001b[39m samples\n\u001b[0;32m---> 43\u001b[0m     batched_graphs \u001b[39m=\u001b[39m dgl\u001b[39m.\u001b[39;49mbatch(graphs)\n\u001b[1;32m     44\u001b[0m     \u001b[39m# batched_labels = [graph.ndata[\"labels\"] for graph in graphs]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     batched_labels \u001b[39m=\u001b[39m batched_graphs\u001b[39m.\u001b[39mndata[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/batch.py:173\u001b[0m, in \u001b[0;36mbatch\u001b[0;34m(graphs, ndata, edata)\u001b[0m\n\u001b[1;32m    170\u001b[0m ntype_ids \u001b[39m=\u001b[39m [graphs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_ntype_id(n) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m ntypes]\n\u001b[1;32m    171\u001b[0m etypes \u001b[39m=\u001b[39m [etype \u001b[39mfor\u001b[39;00m _, etype, _ \u001b[39min\u001b[39;00m relations]\n\u001b[0;32m--> 173\u001b[0m gidx \u001b[39m=\u001b[39m disjoint_union(\n\u001b[1;32m    174\u001b[0m     graphs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mmetagraph, [g\u001b[39m.\u001b[39;49m_graph \u001b[39mfor\u001b[39;49;00m g \u001b[39min\u001b[39;49;00m graphs]\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    176\u001b[0m retg \u001b[39m=\u001b[39m DGLGraph(gidx, ntypes, etypes)\n\u001b[1;32m    178\u001b[0m \u001b[39m# Compute batch num nodes\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/heterograph_index.py:1435\u001b[0m, in \u001b[0;36mdisjoint_union\u001b[0;34m(metagraph, graphs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdisjoint_union\u001b[39m(metagraph, graphs):\n\u001b[1;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a disjoint union of the input heterographs.\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \n\u001b[1;32m   1423\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[39m        Batched Heterograph.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1435\u001b[0m     \u001b[39mreturn\u001b[39;00m _CAPI_DGLHeteroDisjointUnion_v2(metagraph, graphs)\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:296\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.FunctionBase.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./function.pxi:172\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.make_ret\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mdgl/_ffi/_cython/./object.pxi:25\u001b[0m, in \u001b[0;36mdgl._ffi._cy3.core.make_ret_object\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/heterograph_index.py:27\u001b[0m, in \u001b[0;36mHeteroGraphIndex.__new__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m@register_object\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgraph.HeteroGraph\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mHeteroGraphIndex\u001b[39;00m(ObjectBase):\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"HeteroGraph index object.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m    Note\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    ----\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    Do not create GraphIndex directly.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[1;32m     28\u001b[0m         obj \u001b[39m=\u001b[39m ObjectBase\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m)\n\u001b[1;32m     29\u001b[0m         obj\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"# basic training loop\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "import numpy as np\n",
    "\n",
    "# move model to cpu\n",
    "model = model.cpu()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "dataloader = dm.train_dataloader()\n",
    "\n",
    "for epoch in range(50):\n",
    "    training_loss_list = []\n",
    "    with tqdm(dataloader) as tq:\n",
    "        model.train()\n",
    "        r2_list = []\n",
    "        tq.set_description(f\"Epoch {epoch+1}\")\n",
    "        training_loss = 0\n",
    "        for step, (batch_graph, batch_label) in enumerate(tq):\n",
    "            # forward propagation by using all nodes and extracting the user embeddings\n",
    "            batch_graph, batch_label = next(iter(dataloader))\n",
    "            labels = batch_label[\"global\"]\n",
    "            logits = model.forward(batch_graph, batch_graph.ndata[\"feat\"])\n",
    "            loss = F.mse_loss(logits, labels)\n",
    "            training_loss_list.append(loss.item())\n",
    "            # loss_mae = F.l1_loss(logits, labels)\n",
    "            # compute r2 score\n",
    "            r2 = r2_score(logits.detach().numpy(), labels.detach().numpy())\n",
    "            r2_list.append(r2)\n",
    "            # Compute validation accuracy.  Omitted in this example.\n",
    "            # backward propagation\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            training_loss += loss.item()\n",
    "            # tq.set_postfix({\"Step\": step, \"MSE\": loss.item()})\n",
    "\n",
    "        r2_mean = np.mean(r2_list)\n",
    "        loss = np.mean(training_loss_list)\n",
    "        tq.set_postfix({\"final_t_loss\": training_loss, \"R_2\": r2_mean})\n",
    "        print(r2_mean, loss)\n",
    "\n",
    "        # tq.update()\n",
    "        tq.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtaim_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
