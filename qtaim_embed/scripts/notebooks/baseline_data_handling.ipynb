{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "from qtaim_embed.utils.grapher import get_grapher\n",
    "from qtaim_embed.data.molwrapper import mol_wrappers_from_df\n",
    "from qtaim_embed.utils.tests import get_data\n",
    "from qtaim_embed.core.dataset import HeteroGraphNodeLabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/santiagovargas/dev/qtaim_embed/data/qm8/molecules_full.pkl\"\n",
    "df = pd.read_pickle(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21786/21786 [00:02<00:00, 10442.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element set {'O', 'N', 'C', 'H', 'F'}\n",
      "selected keys ['extra_feat_atom_esp_total']\n"
     ]
    }
   ],
   "source": [
    "atom_keys = [\n",
    "    \"extra_feat_atom_esp_total\",\n",
    "]\n",
    "bond_keys = [\n",
    "    \"extra_feat_bond_esp_total\",\n",
    "    \"extra_feat_bond_esp_nuc\",\n",
    "    \"bond_length\",\n",
    "]\n",
    "\n",
    "mol_wrappers, element_set = mol_wrappers_from_df(df, atom_keys, bond_keys)\n",
    "\n",
    "grapher = get_grapher(\n",
    "    element_set,\n",
    "    atom_keys=atom_keys,\n",
    "    bond_keys=bond_keys,\n",
    "    global_keys=[],\n",
    "    allowed_ring_size=[3, 4, 5, 6, 7],\n",
    "    allowed_charges=None,\n",
    "    self_loop=True,\n",
    ")\n",
    "\n",
    "graph_list = []\n",
    "print(\"... Building graphs and featurizing\")\n",
    "for mol in tqdm(mol_wrappers):\n",
    "    graph = grapher.build_graph(mol)\n",
    "    graph, names = grapher.featurize(graph, mol, ret_feat_names=True)\n",
    "    graph_list.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included in labels\n",
      "{'atom': ['extra_feat_atom_esp_total'], 'bond': ['extra_feat_bond_esp_total'], 'global': []}\n",
      "included in graph features\n",
      "{'atom': ['total_degree', 'total_H', 'is_in_ring', 'ring_size_3', 'ring_size_4', 'ring_size_5', 'ring_size_6', 'ring_size_7', 'chemical_symbol_O', 'chemical_symbol_N', 'chemical_symbol_C', 'chemical_symbol_H', 'chemical_symbol_F'], 'bond': ['metal bond', 'ring inclusion', 'ring size_3', 'ring size_4', 'ring size_5', 'ring size_6', 'ring size_7', 'bond_length', 'extra_feat_bond_esp_nuc'], 'global': ['num atoms', 'num bonds', 'molecule weight']}\n",
      "... > loaded dataset\n"
     ]
    }
   ],
   "source": [
    "# TODO: build dataloader class\n",
    "\n",
    "\n",
    "target_dict = {\n",
    "    \"atom\": [\"extra_feat_atom_esp_total\"],\n",
    "    \"bond\": [\"extra_feat_bond_esp_total\"],\n",
    "}\n",
    "\n",
    "graph_list_temp = deepcopy(graph_list)\n",
    "extra_info = {\n",
    "    \"allowed_ring_size\": [3, 4, 5, 6, 7],\n",
    "    \"element_set\": element_set,\n",
    "}\n",
    "train_dataset = HeteroGraphNodeLabelDataset(\n",
    "    mol_wrappers,\n",
    "    graph_list_temp,\n",
    "    names,\n",
    "    target_dict=target_dict,\n",
    "    extra_dataset_info=extra_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "\n",
    "\n",
    "class DataLoaderMoleculeNodeTask(DataLoader):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        if \"collate_fn\" in kwargs:\n",
    "            raise ValueError(\n",
    "                \"'collate_fn' provided internally by 'bondnet.data', you need not to \"\n",
    "                \"provide one\"\n",
    "            )\n",
    "\n",
    "        def collate(samples):\n",
    "            graphs = samples\n",
    "\n",
    "            count_label_atom = 0\n",
    "            for i in graphs:\n",
    "                count_label_atom = count_label_atom + i.ndata[\"labels\"][\"bond\"].shape[0]\n",
    "            batched_graphs = dgl.batch(graphs)\n",
    "            batched_labels = batched_graphs.ndata[\"labels\"]\n",
    "            return batched_graphs, batched_labels\n",
    "\n",
    "        super(DataLoaderMoleculeNodeTask, self).__init__(\n",
    "            dataset, collate_fn=collate, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoaderMoleculeNodeTask(train_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_graph, batch_label = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 13])\n",
      "torch.Size([12, 9])\n",
      "torch.Size([1, 3])\n",
      "14\n",
      "10\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(graph.ndata[\"feat\"][\"atom\"].shape)\n",
    "print(graph.ndata[\"feat\"][\"bond\"].shape)\n",
    "print(graph.ndata[\"feat\"][\"global\"].shape)\n",
    "print(len(grapher.atom_featurizer._feature_name))\n",
    "print(len(grapher.bond_featurizer._feature_name))\n",
    "print(len(grapher.global_featurizer._feature_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn.pytorch as dglnn\n",
    "\n",
    "atom_input_size = int(len(grapher.atom_featurizer._feature_name)) - 1\n",
    "bond_input_size = int(len(grapher.bond_featurizer._feature_name)) - 1\n",
    "global_input_size = 3\n",
    "\n",
    "\n",
    "class testmodel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = dglnn.HeteroGraphConv(\n",
    "            {\n",
    "                \"a2b\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"b2a\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=atom_input_size,\n",
    "                ),\n",
    "                \"a2g\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "                \"g2a\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=atom_input_size,\n",
    "                ),\n",
    "                \"b2g\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "                \"g2b\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"a2a\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=atom_input_size,\n",
    "                ),\n",
    "                \"b2b\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"g2g\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "            },\n",
    "            aggregate=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.conv2 = dglnn.HeteroGraphConv(\n",
    "            {\n",
    "                \"a2b\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"b2a\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=atom_input_size,\n",
    "                ),\n",
    "                \"a2g\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "                \"g2a\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=atom_input_size,\n",
    "                ),\n",
    "                \"b2g\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "                \"g2b\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"a2a\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=atom_input_size,\n",
    "                ),\n",
    "                \"b2b\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"g2g\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "            },\n",
    "            aggregate=\"sum\",\n",
    "        )\n",
    "\n",
    "        self.conv3 = dglnn.HeteroGraphConv(\n",
    "            {\n",
    "                \"b2a\": dglnn.GraphConv(in_feats=bond_input_size, out_feats=1),\n",
    "                \"g2a\": dglnn.GraphConv(in_feats=global_input_size, out_feats=1),\n",
    "                \"a2a\": dglnn.GraphConv(in_feats=atom_input_size, out_feats=1),\n",
    "                \"b2b\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"g2g\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "                \"b2g\": dglnn.GraphConv(\n",
    "                    in_feats=bond_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "                \"g2b\": dglnn.GraphConv(\n",
    "                    in_feats=global_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"a2b\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=bond_input_size,\n",
    "                ),\n",
    "                \"a2g\": dglnn.GraphConv(\n",
    "                    in_feats=atom_input_size,\n",
    "                    out_feats=global_input_size,\n",
    "                ),\n",
    "            },\n",
    "            aggregate=\"sum\",\n",
    "        )\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        feats = self.conv(graph, inputs)\n",
    "        feats = self.conv2(graph, feats)\n",
    "        feats = self.conv3(graph, feats)\n",
    "        return feats\n",
    "\n",
    "\n",
    "testmodel = testmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_out = testmodel(graph, graph.ndata[\"feat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1085/1085 [00:13<00:00, 81.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-464.0406035106687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1085/1085 [00:13<00:00, 80.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3549.6973953827423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1085/1085 [00:13<00:00, 81.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1779.1195338412172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1085/1085 [00:13<00:00, 80.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1557.6191949635447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1085/1085 [00:13<00:00, 80.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-744.0677690230102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1085/1085 [00:13<00:00, 82.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-910.7186760316185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1085/1085 [00:13<00:00, 81.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-780.6937679869957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1085/1085 [00:13<00:00, 80.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1862.3083102921316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1085/1085 [00:13<00:00, 81.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-421.9139219835718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1085/1085 [00:13<00:00, 81.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-344.26083965853707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1085/1085 [00:13<00:00, 82.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-180.26670257604817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:  84%|████████▍ | 914/1085 [00:11<00:02, 81.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m batch_graph, batch_label \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloader))\n\u001b[1;32m     19\u001b[0m labels \u001b[39m=\u001b[39m batch_label[\u001b[39m\"\u001b[39m\u001b[39matom\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m logits \u001b[39m=\u001b[39m testmodel(batch_graph, batch_graph\u001b[39m.\u001b[39;49mndata[\u001b[39m\"\u001b[39;49m\u001b[39mfeat\u001b[39;49m\u001b[39m\"\u001b[39;49m])[\u001b[39m\"\u001b[39m\u001b[39matom\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[39m# compute loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(logits, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[94], line 137\u001b[0m, in \u001b[0;36mtestmodel.forward\u001b[0;34m(self, graph, inputs)\u001b[0m\n\u001b[1;32m    135\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(graph, inputs)\n\u001b[1;32m    136\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(graph, feats)\n\u001b[0;32m--> 137\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(graph, feats)\n\u001b[1;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m feats\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/nn/pytorch/hetero.py:210\u001b[0m, in \u001b[0;36mHeteroGraphConv.forward\u001b[0;34m(self, g, inputs, mod_args, mod_kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m stype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m inputs:\n\u001b[1;32m    209\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m         dstdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module((stype, etype, dtype))(\n\u001b[1;32m    211\u001b[0m             rel_graph,\n\u001b[1;32m    212\u001b[0m             (inputs[stype], inputs[dtype]),\n\u001b[1;32m    213\u001b[0m             \u001b[39m*\u001b[39;49mmod_args\u001b[39m.\u001b[39;49mget(etype, ()),\n\u001b[1;32m    214\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmod_kwargs\u001b[39m.\u001b[39;49mget(etype, {})\n\u001b[1;32m    215\u001b[0m         )\n\u001b[1;32m    216\u001b[0m         outputs[dtype]\u001b[39m.\u001b[39mappend(dstdata)\n\u001b[1;32m    217\u001b[0m rsts \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/nn/pytorch/conv/graphconv.py:428\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    426\u001b[0m feat_src, feat_dst \u001b[39m=\u001b[39m expand_as_pair(feat, graph)\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_norm \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 428\u001b[0m     degs \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49mout_degrees()\u001b[39m.\u001b[39mto(feat_src)\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    429\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_norm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    430\u001b[0m         norm \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mpow(degs, \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/heterograph.py:3677\u001b[0m, in \u001b[0;36mDGLGraph.out_degrees\u001b[0;34m(self, u, etype)\u001b[0m\n\u001b[1;32m   3674\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3675\u001b[0m         \u001b[39mreturn\u001b[39;00m deg\n\u001b[0;32m-> 3677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mout_degrees\u001b[39m(\u001b[39mself\u001b[39m, u\u001b[39m=\u001b[39mALL, etype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   3678\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the out-degree(s) of the given nodes.\u001b[39;00m\n\u001b[1;32m   3679\u001b[0m \n\u001b[1;32m   3680\u001b[0m \u001b[39m    It computes the out-degree(s) w.r.t. to the edges of the given edge type.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[39m    in_degrees\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3742\u001b[0m     srctype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_canonical_etype(etype)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "opt = torch.optim.Adam(testmodel.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    with tqdm(dataloader) as tq:\n",
    "        testmodel.train()\n",
    "        r2_list = []\n",
    "        tq.set_description(f\"Epoch {epoch+1}\")\n",
    "        training_loss = 0\n",
    "        for step, (batch_graph, batch_label) in enumerate(tq):\n",
    "            # forward propagation by using all nodes and extracting the user embeddings\n",
    "            batch_graph, batch_label = next(iter(dataloader))\n",
    "            labels = batch_label[\"atom\"]\n",
    "            logits = testmodel(batch_graph, batch_graph.ndata[\"feat\"])[\"atom\"]\n",
    "\n",
    "            # compute loss\n",
    "            loss = F.mse_loss(logits, labels)\n",
    "            # loss_mae = F.l1_loss(logits, labels)\n",
    "            # compute r2 score\n",
    "            r2 = r2_score(logits.detach().numpy(), labels.detach().numpy())\n",
    "            r2_list.append(r2)\n",
    "            # Compute validation accuracy.  Omitted in this example.\n",
    "            # backward propagation\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            training_loss += loss.item()\n",
    "            # tq.set_postfix({\"Step\": step, \"MSE\": loss.item()})\n",
    "\n",
    "        r2_mean = np.mean(r2_list)\n",
    "        tq.set_postfix({\"final_t_loss\": training_loss, \"R_2\": r2_mean})\n",
    "        print(r2_mean)\n",
    "\n",
    "        # tq.update()\n",
    "        tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [00:09<00:00, 109.67it/s]\n"
     ]
    }
   ],
   "source": [
    "label_list = []\n",
    "predictions_list = []\n",
    "\n",
    "with tqdm(dataloader) as tq, torch.no_grad():\n",
    "    for step, (batch_graph, batch_label) in enumerate(tq):\n",
    "        batch_graph, batch_label = next(iter(dataloader))\n",
    "        labels = batch_label[\"atom\"]\n",
    "        logits = testmodel(batch_graph, batch_graph.ndata[\"feat\"])[\"atom\"]\n",
    "\n",
    "        label_list.append(labels.cpu().numpy())\n",
    "        predictions_list.append(logits.cpu().numpy())\n",
    "\n",
    "\n",
    "cat_labels = np.concatenate(label_list)\n",
    "cat_preds = np.concatenate(predictions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006468550770712955\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(cat_labels, cat_preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtaim_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
