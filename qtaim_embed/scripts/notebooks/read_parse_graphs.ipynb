{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "from tqdm import tqdm\n",
    "from qtaim_embed.utils.grapher import get_grapher\n",
    "from qtaim_embed.data.molwrapper import mol_wrappers_from_df\n",
    "from qtaim_embed.utils.tests import get_data\n",
    "from qtaim_embed.core.dataset import HeteroGraphNodeLabelDataset\n",
    "from qtaim_embed.data.dataloader import DataLoaderMoleculeNodeTask\n",
    "from qtaim_embed.models.node_level.base_gnn import GCNNodePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > creating MoleculeWrapper objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21786/21786 [00:06<00:00, 3557.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element set {'N', 'F', 'O', 'H', 'C'}\n",
      "selected atomic keys ['extra_feat_atom_esp_total']\n",
      "selected bond keys ['extra_feat_bond_esp_total', 'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta', 'bond_length']\n",
      "selected global keys []\n",
      "... > Building graphs and featurizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21785/21785 [02:26<00:00, 148.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included in labels\n",
      "{'atom': ['extra_feat_atom_esp_total'], 'bond': ['extra_feat_bond_esp_total', 'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta'], 'global': []}\n",
      "included in graph features\n",
      "{'atom': ['total_degree', 'total_H', 'is_in_ring', 'ring_size_3', 'ring_size_4', 'ring_size_5', 'ring_size_6', 'ring_size_7', 'chemical_symbol_N', 'chemical_symbol_F', 'chemical_symbol_O', 'chemical_symbol_H', 'chemical_symbol_C'], 'bond': ['metal bond', 'ring inclusion', 'ring size_3', 'ring size_4', 'ring size_5', 'ring size_6', 'ring size_7', 'bond_length'], 'global': ['num atoms', 'num bonds', 'molecule weight']}\n",
      "... > parsing labels and features in graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21785/21785 [00:09<00:00, 2412.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > Scaling features\n",
      "Standard deviation for feature 0 is 0.0, smaller than 0.001. You may want to exclude this feature.\n",
      "... > Scaling features complete\n",
      "... > mean: \n",
      " {'atom': tensor([2.0525e+00, 5.2508e-01, 2.5562e-01, 5.0526e-02, 5.3274e-02, 9.2300e-02,\n",
      "        5.0783e-02, 8.7325e-03, 5.9473e-02, 1.4606e-03, 7.8644e-02, 5.1693e-01,\n",
      "        3.4349e-01]), 'bond': tensor([0.0000, 0.2584, 0.0523, 0.0571, 0.0956, 0.0543, 0.0101, 1.2687]), 'global': tensor([ 16.0904,  16.5128, 108.8643])}\n",
      "... > std:  \n",
      " {'atom': tensor([1.2716, 0.8735, 0.4362, 0.2190, 0.2246, 0.2894, 0.2196, 0.0930, 0.2365,\n",
      "        0.0382, 0.2692, 0.4997, 0.4749]), 'bond': tensor([0.0010, 0.4377, 0.2226, 0.2320, 0.2941, 0.2267, 0.1002, 0.2146]), 'global': tensor([2.9101, 3.1555, 8.0615])}\n",
      "... > Scaling targets\n",
      "... > Scaling targets complete\n",
      "... > mean: \n",
      " {'atom': tensor([2260655.2500]), 'bond': tensor([0.9732, 0.0857, 1.4807])}\n",
      "... > std:  \n",
      " {'atom': tensor([63750496.]), 'bond': tensor([0.4235, 0.2129, 0.4297])}\n",
      "... > loaded dataset\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"/home/santiagovargas/dev/qtaim_embed/data/xyz_qm8/molecules_qtaim.pkl\")\n",
    "\n",
    "train_dataset = HeteroGraphNodeLabelDataset(\n",
    "    #file=\"/home/santiagovargas/dev/qtaim_embed/data/xyz_qm8/molecules_full.pkl\",\n",
    "    file=\"/home/santiagovargas/dev/qtaim_embed/data/xyz_qm8/molecules_qtaim.pkl\",\n",
    "    allowed_ring_size=[3, 4, 5, 6, 7],\n",
    "    allowed_charges=None,\n",
    "    self_loop=True,\n",
    "    extra_keys={\n",
    "        \"atom\": [\"extra_feat_atom_esp_total\"],\n",
    "        \"bond\": [\n",
    "            \"extra_feat_bond_esp_total\",\n",
    "            'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta',\n",
    "            \"bond_length\",\n",
    "        ],\n",
    "        \"global\": [],\n",
    "    },\n",
    "    target_dict={\n",
    "        \"atom\": [\"extra_feat_atom_esp_total\"],\n",
    "        \"bond\": [\"extra_feat_bond_esp_total\", 'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta',],\n",
    "    },\n",
    "    extra_dataset_info={},\n",
    "    debug=False,\n",
    "    log_scale_targets=False,\n",
    "    standard_scale_targets=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output dims 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len_dict = train_dataset.featuze_size()\n",
    "atom_input_size = len_dict[\"atom\"]\n",
    "bond_input_size = len_dict[\"bond\"]\n",
    "global_input_size = len_dict[\"global\"]\n",
    "\n",
    "model = GCNNodePred(\n",
    "    atom_input_size=atom_input_size,\n",
    "    bond_input_size=bond_input_size,\n",
    "    global_input_size=global_input_size,\n",
    "    n_conv_layers=2,\n",
    "    target_dict={\n",
    "        \"atom\": [\"extra_feat_atom_esp_total\"],\n",
    "        \"bond\": [\"extra_feat_bond_esp_total\", 'extra_feat_bond_ellip_e_dens', 'extra_feat_bond_eta',],\n",
    "    },\n",
    "    activation=\"ReLU\",\n",
    "    dropout=0.2,\n",
    "    lr_plateau_patience=10,\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoaderMoleculeNodeTask(train_dataset, batch_size=100, shuffle=True)\n",
    "batch_graph, batch_label = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfeats = batch_graph.ndata[\"feat\"]\\nfor layer in model.conv_layers:\\n    feats = layer(batch_graph, feats)\\n\\n\\nfor key in [\"atom\", \"bond\", \"global\"]:\\n    feats[key] = _split_batched_output(batch_graph, feats[key], key)\\n\\n\\ndef get_targets(self, targets_feats):\\n    targets = {}\\n    for k, v in self.target_dict.items():\\n        # if v is None or [] skip \\n        if not (v is None or len(v) == 0):\\n            targets[k] = targets_feats[k]\\n    #[print(i) for i in list(targets.values())]\\n    # concat dict of tensors into one tensor\\n    list(targets.values())\\n    targets = torch.cat(list(targets.values()), dim=1)\\n    return targets    \\n\\ntargets = get_targets(model, feats)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "feats = batch_graph.ndata[\"feat\"]\n",
    "for layer in model.conv_layers:\n",
    "    feats = layer(batch_graph, feats)\n",
    "\n",
    "\n",
    "for key in [\"atom\", \"bond\", \"global\"]:\n",
    "    feats[key] = _split_batched_output(batch_graph, feats[key], key)\n",
    "\n",
    "\n",
    "def get_targets(self, targets_feats):\n",
    "    targets = {}\n",
    "    for k, v in self.target_dict.items():\n",
    "        # if v is None or [] skip \n",
    "        if not (v is None or len(v) == 0):\n",
    "            targets[k] = targets_feats[k]\n",
    "    #[print(i) for i in list(targets.values())]\n",
    "    # concat dict of tensors into one tensor\n",
    "    list(targets.values())\n",
    "    targets = torch.cat(list(targets.values()), dim=1)\n",
    "    return targets    \n",
    "\n",
    "targets = get_targets(model, feats)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = model.forward(batch_graph, batch_graph.ndata[\"feat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atom': tensor([[ 4.3852],\n",
       "         [ 5.0082],\n",
       "         [ 9.4321],\n",
       "         ...,\n",
       "         [-0.6390],\n",
       "         [-1.1658],\n",
       "         [-0.7524]], grad_fn=<SumBackward1>),\n",
       " 'bond': tensor([[ 0.2367,  0.2887,  0.4309],\n",
       "         [-1.1728, -0.0552, -1.5208],\n",
       "         [-0.9402, -1.5262, -1.6328],\n",
       "         ...,\n",
       "         [ 1.1535, -1.5262, -1.6328],\n",
       "         [-0.4272,  3.1284,  1.2104],\n",
       "         [ 0.0921, -0.7483, -1.6328]], grad_fn=<SumBackward1>),\n",
       " 'global': tensor([[-1.8912e+00, -2.7998e-01,  2.4384e+00],\n",
       "         [-4.6431e-01,  3.7792e+00, -5.4131e-01],\n",
       "         [ 2.3665e-01, -1.1738e+00,  1.9950e+00],\n",
       "         [ 2.9629e+00, -1.4526e+00,  3.5168e+00],\n",
       "         [ 1.8827e+00, -1.5585e+00,  6.7825e-01],\n",
       "         [-5.7771e-01, -1.5585e+00,  1.4813e+00],\n",
       "         [ 3.5461e+00,  1.3334e+00,  2.7724e+00],\n",
       "         [-2.8486e-01,  6.9395e-01, -1.1819e+00],\n",
       "         [-2.3902e-01, -1.2880e+00, -1.8501e+00],\n",
       "         [ 2.9359e+00,  7.3711e-01,  3.5059e+00],\n",
       "         [-1.3242e+00, -1.5585e+00, -1.8501e+00],\n",
       "         [ 1.0949e+00, -1.5585e+00,  2.1539e+00],\n",
       "         [-4.9099e-01,  1.6668e-02, -1.6386e+00],\n",
       "         [ 2.5519e-01, -2.7174e-01, -1.7387e-01],\n",
       "         [-9.1965e-01, -2.3709e-01, -1.8501e+00],\n",
       "         [ 1.9152e-01, -1.3128e+00, -5.5728e-01],\n",
       "         [ 3.1823e+00, -1.3853e+00, -1.8501e+00],\n",
       "         [ 1.5351e+00, -8.7380e-01,  4.4153e-01],\n",
       "         [ 2.0393e+00,  6.3502e-01, -1.6080e+00],\n",
       "         [ 2.6454e+00, -1.2008e+00,  3.1506e+00],\n",
       "         [-1.8912e+00,  2.6978e+00, -8.2471e-01],\n",
       "         [-9.6522e-01, -1.5585e+00, -1.5508e-03],\n",
       "         [-5.6844e-01, -1.8721e-01, -1.5941e+00],\n",
       "         [-1.8912e+00,  5.2799e+00,  2.0038e+00],\n",
       "         [ 2.6491e+00, -1.5585e+00, -9.3984e-01],\n",
       "         [-1.0983e+00, -1.5585e+00, -1.8501e+00],\n",
       "         [-1.8912e+00, -1.5585e+00, -7.2579e-01],\n",
       "         [-1.8912e+00,  2.5407e-02, -1.8501e+00],\n",
       "         [-5.4627e-01, -1.5585e+00,  5.5510e-01],\n",
       "         [ 4.0950e+00,  7.3332e+00, -1.8501e+00],\n",
       "         [ 4.2187e+00, -1.5585e+00, -1.8501e+00],\n",
       "         [ 1.2917e+00,  4.8442e-01, -1.8501e+00],\n",
       "         [-1.6331e+00,  3.6863e+00,  1.9102e+00],\n",
       "         [-1.8912e+00, -1.4397e+00, -1.7398e+00],\n",
       "         [-8.9046e-01, -1.2493e+00, -8.5673e-01],\n",
       "         [ 3.4939e+00,  6.0238e-01, -1.1957e+00],\n",
       "         [-1.1134e+00, -1.3217e+00, -1.0928e+00],\n",
       "         [-2.9826e-01,  2.8237e-01, -1.7737e+00],\n",
       "         [-1.6245e+00, -1.5585e+00, -1.2537e+00],\n",
       "         [ 2.4799e+00,  3.1301e+00, -1.8501e+00],\n",
       "         [ 1.9133e-01, -1.4551e+00, -1.8501e+00],\n",
       "         [-1.7380e+00,  2.8821e+00, -3.7556e-01],\n",
       "         [-1.8912e+00, -1.5585e+00, -8.0513e-01],\n",
       "         [-5.5744e-01,  3.4935e-01, -1.6005e+00],\n",
       "         [ 1.6109e+00,  8.5291e-01,  6.1351e+00],\n",
       "         [-1.3195e+00, -1.0817e+00, -2.6468e-01],\n",
       "         [-1.8912e+00,  3.3822e+00,  2.3331e+00],\n",
       "         [ 3.3573e+00, -3.4039e-01,  5.6921e+00],\n",
       "         [-1.8912e+00,  4.2657e+00, -4.2134e-01],\n",
       "         [-1.8912e+00, -1.5581e+00,  4.2334e+00],\n",
       "         [-1.2780e+00, -1.5585e+00,  1.1144e+00],\n",
       "         [-1.8912e+00, -2.4410e-01, -1.8501e+00],\n",
       "         [ 2.3029e+00,  2.3663e+00, -1.8501e+00],\n",
       "         [-7.9534e-01, -1.2966e+00, -1.8501e+00],\n",
       "         [-1.7380e+00, -1.5585e+00, -1.8501e+00],\n",
       "         [ 2.5791e-01, -1.0438e+00, -2.6139e-01],\n",
       "         [-7.2641e-01, -9.9763e-01,  9.4558e-02],\n",
       "         [-1.6663e+00, -1.5585e+00, -1.8501e+00],\n",
       "         [ 1.5102e+00,  6.5878e-01, -1.8501e+00],\n",
       "         [-1.8912e+00, -1.5585e+00,  6.5133e-01],\n",
       "         [ 4.5385e+00, -1.5585e+00,  2.8205e-01],\n",
       "         [ 2.1845e+00,  1.1243e+00, -1.3754e+00],\n",
       "         [-1.8912e+00,  3.0819e+00,  2.1422e+00],\n",
       "         [ 5.6001e-01, -1.3103e+00, -9.0769e-01],\n",
       "         [ 1.7696e+00,  1.0756e+00, -1.8501e+00],\n",
       "         [ 1.9525e-01, -9.8103e-01,  5.8220e-01],\n",
       "         [-9.0134e-01, -8.2805e-02,  2.5118e+00],\n",
       "         [-1.8912e+00,  2.2198e+00, -1.2460e+00],\n",
       "         [-1.8912e+00,  2.8083e+00,  6.7355e-01],\n",
       "         [-1.6444e+00, -1.3183e+00, -1.3004e+00],\n",
       "         [-1.8912e+00, -1.1134e+00,  7.7563e-01],\n",
       "         [-1.8912e+00,  2.1377e+00,  1.8349e+00],\n",
       "         [-1.8912e+00, -1.2310e+00,  7.2240e-01],\n",
       "         [ 1.1069e+00, -1.2638e+00,  1.0613e+00],\n",
       "         [ 2.4805e+00, -2.8560e-01,  5.7188e-01],\n",
       "         [ 1.8102e+00,  2.4300e+00,  4.1343e+00],\n",
       "         [-1.3697e+00,  1.6389e+00, -1.8501e+00],\n",
       "         [ 1.1212e+00, -8.9490e-01,  3.2428e+00],\n",
       "         [-4.1159e-01, -1.0269e+00, -1.8501e+00],\n",
       "         [-1.6411e+00, -1.3163e+00, -1.2793e+00],\n",
       "         [ 7.1456e-01, -7.6916e-01,  3.4940e+00],\n",
       "         [-1.8912e+00,  2.4752e+00, -1.1390e+00],\n",
       "         [-1.8912e+00, -1.5585e+00, -6.9372e-01],\n",
       "         [-4.0846e-01, -7.4731e-01,  1.5111e+00],\n",
       "         [ 1.4138e+00,  1.0035e+00,  2.0653e+00],\n",
       "         [-6.7392e-01,  1.5910e+00,  4.0068e-01],\n",
       "         [ 1.0871e+00,  7.3047e-01, -1.3295e+00],\n",
       "         [-2.9376e-01,  1.2080e+00,  2.5985e+00],\n",
       "         [-1.8912e+00, -1.5585e+00, -1.8501e+00],\n",
       "         [ 1.0016e+00, -1.2133e+00,  1.1104e+00],\n",
       "         [-1.0549e+00, -1.5585e+00,  2.0842e-01],\n",
       "         [-5.7617e-01, -1.5585e+00, -1.6473e+00],\n",
       "         [-1.8912e+00,  2.5763e+00, -1.8501e+00],\n",
       "         [ 8.7573e-01, -1.5585e+00,  1.6716e+00],\n",
       "         [-1.8912e+00,  9.3472e-01, -1.5772e+00],\n",
       "         [-7.4927e-01,  1.6387e-01, -1.8501e+00],\n",
       "         [ 8.1017e-01, -1.1767e+00, -4.0531e-02],\n",
       "         [ 5.4361e-01,  6.5383e-02, -1.6482e+00],\n",
       "         [ 2.1918e+00, -1.2329e+00, -6.4627e-02],\n",
       "         [ 1.7073e+00, -7.9722e-01, -2.2370e-01]], grad_fn=<SumBackward1>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimizer, lr_scheduler = model.configure_optimizers()\\noptimizer = optimizer[0]\\nlr_scheduler = lr_scheduler[0]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "optimizer, lr_scheduler = model.configure_optimizers()\n",
    "optimizer = optimizer[0]\n",
    "lr_scheduler = lr_scheduler[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name            | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0  | conv_layers     | ModuleList         | 1.0 K \n",
      "1  | loss            | MultioutputWrapper | 0     \n",
      "2  | train_r2        | MultioutputWrapper | 0     \n",
      "3  | train_torch_l1  | MultioutputWrapper | 0     \n",
      "4  | train_torch_mse | MultioutputWrapper | 0     \n",
      "5  | val_r2          | MultioutputWrapper | 0     \n",
      "6  | val_torch_l1    | MultioutputWrapper | 0     \n",
      "7  | val_torch_mse   | MultioutputWrapper | 0     \n",
      "8  | test_r2         | MultioutputWrapper | 0     \n",
      "9  | test_torch_l1   | MultioutputWrapper | 0     \n",
      "10 | test_torch_mse  | MultioutputWrapper | 0     \n",
      "--------------------------------------------------------\n",
      "1.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/218 [00:00<?, ?it/s] "
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer_transfer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=3.0,\n",
    "    default_root_dir=\"./test/\",\n",
    "    precision=32,\n",
    ")\n",
    "\n",
    "trainer_transfer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 24.94it/s, Step=0, MSE=14.5]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00, 27.28it/s, Step=0, MSE=14.2]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 28.43it/s, Step=0, MSE=14.6]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 24.78it/s, Step=0, MSE=14.1]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00, 25.01it/s, Step=0, MSE=13.9]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s, Step=0, MSE=14.4]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00, 24.11it/s, Step=0, MSE=14]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00, 16.64it/s, Step=0, MSE=14.2]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 28.94it/s, Step=0, MSE=14.8]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00, 28.79it/s, Step=0, MSE=13.7]\n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00, 25.83it/s, Step=0, MSE=13.9]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00, 27.60it/s, Step=0, MSE=14.5]\n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00, 29.53it/s, Step=0, MSE=14.4]\n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00, 28.14it/s, Step=0, MSE=14.9]\n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00, 27.34it/s, Step=0, MSE=14.7]\n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00, 28.24it/s, Step=0, MSE=14.8]\n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00, 26.58it/s, Step=0, MSE=14.9]\n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00, 27.29it/s, Step=0, MSE=13.7]\n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00, 26.58it/s, Step=0, MSE=13.4]\n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00, 27.06it/s, Step=0, MSE=14.5]\n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00, 26.45it/s, Step=0, MSE=13.8]\n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00, 26.29it/s, Step=0, MSE=14.7]\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 26.35it/s, Step=0, MSE=14.9]\n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s, Step=0, MSE=14.3]\n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00, 24.98it/s, Step=0, MSE=13.7]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00, 26.17it/s, Step=0, MSE=14.7]\n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00, 25.34it/s, Step=0, MSE=13.7]\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00, 26.00it/s, Step=0, MSE=14.4]\n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00, 27.53it/s, Step=0, MSE=14.3]\n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00, 27.05it/s, Step=0, MSE=14.3]\n",
      "Epoch 31: 100%|██████████| 1/1 [00:00<00:00, 26.86it/s, Step=0, MSE=13.8]\n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00, 27.68it/s, Step=0, MSE=14.5]\n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00, 28.07it/s, Step=0, MSE=14.2]\n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00, 27.50it/s, Step=0, MSE=14.8]\n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00, 28.42it/s, Step=0, MSE=13.9]\n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00, 27.01it/s, Step=0, MSE=13.9]\n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00, 27.04it/s, Step=0, MSE=13.6]\n",
      "Epoch 38: 100%|██████████| 1/1 [00:00<00:00, 28.62it/s, Step=0, MSE=13.4]\n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00, 27.49it/s, Step=0, MSE=13.3]\n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00, 28.05it/s, Step=0, MSE=13.8]\n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00, 27.27it/s, Step=0, MSE=14.1]\n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00, 26.05it/s, Step=0, MSE=13.7]\n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00, 29.28it/s, Step=0, MSE=13.8]\n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00, 27.95it/s, Step=0, MSE=14.4]\n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00, 27.04it/s, Step=0, MSE=14.9]\n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00, 27.83it/s, Step=0, MSE=14.1]\n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00, 28.02it/s, Step=0, MSE=13.5]\n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00, 28.09it/s, Step=0, MSE=14.3]\n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00, 27.86it/s, Step=0, MSE=13.8]\n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00, 27.97it/s, Step=0, MSE=13.4]\n"
     ]
    }
   ],
   "source": [
    "#opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "from sklearn.metrics import r2_score\n",
    "# import F \n",
    "import torch.nn.functional as F\n",
    "for epoch in range(50):\n",
    "    training_loss_list = []\n",
    "    with tqdm(dataloader) as tq:\n",
    "        model.train()\n",
    "        r2_list = []\n",
    "        tq.set_description(f\"Epoch {epoch+1}\")\n",
    "        training_loss = 0\n",
    "        target_type = \"bond\" \n",
    "\n",
    "        for step, (batch_graph, batch_label) in enumerate(tq):\n",
    "            # forward propagation by using all nodes and extracting the user embeddings\n",
    "            batch_graph, batch_label = next(iter(dataloader))\n",
    "            labels = batch_label[target_type]\n",
    "            logits = model(batch_graph, batch_graph.ndata[\"feat\"])[target_type]\n",
    "            \n",
    "            logits_list = []\n",
    "            labels_list = []\n",
    "            max_nodes = -1\n",
    "            for target_type in [\"bond\", \"atom\"]:\n",
    "                \n",
    "                #print(logits.shape)\n",
    "                #print(labels.shape)\n",
    "                if max_nodes < logits.shape[0]:\n",
    "                    max_nodes = logits.shape[0]\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(labels)\n",
    "\n",
    "                # compute loss\n",
    "            # zero pad logits and labels to max_nodes size\n",
    "            logits_list = [F.pad(i, (0, 0, 0, max_nodes - i.shape[0])) for i in logits_list]\n",
    "            labels_list = [F.pad(i, (0, 0, 0, max_nodes - i.shape[0])) for i in labels_list]\n",
    "            logits = torch.cat(logits_list, dim=1)\n",
    "            labels = torch.cat(labels_list, dim=1)\n",
    "\n",
    "            loss = F.mse_loss(logits, labels)\n",
    "            training_loss_list.append(loss.item())\n",
    "            # loss_mae = F.l1_loss(logits, labels)\n",
    "            # compute r2 score\n",
    "            r2 = r2_score(logits.detach().numpy(), labels.detach().numpy())\n",
    "            r2_list.append(r2)\n",
    "            #print(r2)\n",
    "            # Compute validation accuracy.  Omitted in this example.\n",
    "            # backward propagation\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "            tq.set_postfix({\"Step\": step, \"MSE\": loss.item()})\n",
    "\n",
    "        # show mean training loss\n",
    "        #tq.set_postfix({\"final_t_loss\": training_loss, \"R_2\": r2})\n",
    "        r2_mean = np.mean(r2_list)\n",
    "        loss = np.mean(training_loss_list)\n",
    "        #tq.set_postfix({\"final_t_loss\": training_loss, \"R_2\": r2_mean})\n",
    "        #print(r2_mean, loss)\n",
    "\n",
    "        # tq.update()\n",
    "        tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtaim_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
