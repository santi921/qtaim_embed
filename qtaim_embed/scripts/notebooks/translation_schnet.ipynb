{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qm9data = QM9(\\n    \"./qm9.db\",\\n    batch_size=10,\\n    num_train=110000,\\n    num_val=10000,\\n    transforms=[ASENeighborList(cutoff=5.0)],\\n)\\nqm9data.prepare_data()\\nqm9data.setup()'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from schnetpack.data import ASEAtomsData\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "import schnetpack as spk\n",
    "import schnetpack.transform as trn\n",
    "from schnetpack.datasets import QM9\n",
    "from schnetpack.transform import ASENeighborList\n",
    "\n",
    "\"\"\"qm9data = QM9(\n",
    "    \"./qm9.db\",\n",
    "    batch_size=10,\n",
    "    num_train=110000,\n",
    "    num_val=10000,\n",
    "    transforms=[ASENeighborList(cutoff=5.0)],\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties: {'E1_CC2': [0.1692052], 'E2_CC2': [0.25455461]}\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"./qm8_train_dimenet.npz\")\n",
    "\n",
    "atoms_list = []\n",
    "property_list = []\n",
    "numbers = data[\"N\"]\n",
    "R = data[\"R\"]\n",
    "Z = data[\"Z\"]\n",
    "atom_count = 0\n",
    "for mol_ind, mol in enumerate(numbers):\n",
    "    ats = Atoms(\n",
    "        positions=R[atom_count : atom_count + mol],\n",
    "        numbers=Z[atom_count : atom_count + mol],\n",
    "    )\n",
    "    atoms_list.append(ats)\n",
    "    atom_count += mol\n",
    "    properties = {\n",
    "        \"E1_CC2\": [data[\"E1_CC2\"][mol_ind]],\n",
    "        \"E2_CC2\": [data[\"E2_CC2\"][mol_ind]],\n",
    "    }\n",
    "    property_list.append(properties)\n",
    "\n",
    "print(\"Properties:\", property_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = ASEAtomsData.create(\n",
    "    \"./qm8_train_schnet.db\",\n",
    "    distance_unit=\"Ang\",\n",
    "    property_unit_dict={\"E1_CC2\": \"Hartree\", \"E2_CC2\": \"Hartree\"},\n",
    ")\n",
    "new_dataset.add_systems(property_list, atoms_list)\n",
    "\n",
    "custom_data = spk.data.AtomsDataModule(\n",
    "    \"./qm8_train_schnet.db\",\n",
    "    batch_size=10,\n",
    "    distance_unit=\"Ang\",\n",
    "    property_units={\n",
    "        \"E1_CC2\": \"Hartree\",\n",
    "        # \"E2_CC2\": \"Hartree\",\n",
    "    },\n",
    "    num_train=18607,\n",
    "    num_val=1000,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.0),\n",
    "        trn.CastTo32(),\n",
    "    ],\n",
    "    num_workers=1,\n",
    "    split_file=\"./qm8_split.npz\",\n",
    "    pin_memory=True,  # set to false, when not using a GPU\n",
    "    load_properties=[\"E1_CC2\"],\n",
    ")\n",
    "custom_data.prepare_data()\n",
    "custom_data.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5.0\n",
    "n_atom_basis = 30\n",
    "\n",
    "pairwise_distance = (\n",
    "    spk.atomistic.PairwiseDistances()\n",
    ")  # calculates pairwise distances between atoms\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "schnet = spk.representation.SchNet(\n",
    "    n_atom_basis=n_atom_basis,\n",
    "    n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff),\n",
    ")\n",
    "\n",
    "pred_e1 = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=\"E1_CC2\")\n",
    "\n",
    "painn = spk.representation.PaiNN(\n",
    "    n_atom_basis=n_atom_basis,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff),\n",
    "    n_interactions=3,\n",
    "    activation=spk.nn.activations.shifted_softplus,\n",
    "    shared_interactions=True,\n",
    "    shared_filters=True,\n",
    ")\n",
    "\n",
    "\n",
    "# pred_e2 = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=\"E2_CC2\")\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=painn,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_e1],\n",
    "    postprocessors=[\n",
    "        trn.CastTo64(),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "output_e1 = spk.task.ModelOutput(\n",
    "    name=\"E1_CC2\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1.0,\n",
    "    metrics={\"MAE\": torchmetrics.MeanAbsoluteError(), \"r2\": torchmetrics.R2Score()},\n",
    ")\n",
    "\n",
    "\n",
    "output_e2 = spk.task.ModelOutput(\n",
    "    name=\"E2_CC2\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1.0,\n",
    "    metrics={\"MAE\": torchmetrics.MeanAbsoluteError(), \"r2\": torchmetrics.R2Score()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/schnet/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_e1],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-3},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: ./qm8_test/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 15.5 K\n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/schnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/schnet/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/santiagovargas/anaconda3/envs/schnet/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1861/1861 [00:25<00:00, 71.71it/s, v_num=0, val_loss=0.000689]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1861/1861 [00:25<00:00, 71.71it/s, v_num=0, val_loss=0.000689]\n"
     ]
    }
   ],
   "source": [
    "qm9tut = \"./qm8_test\"\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=qm9tut)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=os.path.join(qm9tut, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=qm9tut,\n",
    "    max_epochs=10,  # for testing, we restrict the number of epochs\n",
    ")\n",
    "trainer.fit(task, datamodule=custom_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./qm8_test_dimenet.npz\")\n",
    "\n",
    "atoms_list = []\n",
    "property_list = []\n",
    "numbers = data[\"N\"]\n",
    "R = data[\"R\"]\n",
    "Z = data[\"Z\"]\n",
    "atom_count = 0\n",
    "for mol_ind, mol in enumerate(numbers):\n",
    "    ats = Atoms(\n",
    "        positions=R[atom_count : atom_count + mol],\n",
    "        numbers=Z[atom_count : atom_count + mol],\n",
    "    )\n",
    "    atoms_list.append(ats)\n",
    "    atom_count += mol\n",
    "    properties = {\n",
    "        \"E1_CC2\": [data[\"E1_CC2\"][mol_ind]],\n",
    "        # \"E2_CC2\": [data[\"E2_CC2\"][mol_ind]],\n",
    "    }\n",
    "    property_list.append(properties)\n",
    "\n",
    "# print(\"Properties:\", property_list[0])\n",
    "\n",
    "new_dataset = ASEAtomsData.create(\n",
    "    \"./qm8_test_schnet.db\",\n",
    "    distance_unit=\"Ang\",\n",
    "    property_unit_dict={\n",
    "        \"E1_CC2\": \"Hartree\",\n",
    "        # \"E2_CC2\": \"Hartree\",\n",
    "    },\n",
    ")\n",
    "new_dataset.add_systems(property_list, atoms_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Split file was given, but `num_train (0.01) != len(train_idx)` (21)!\n",
      "WARNING:root:Split file was given, but `num_val (0.99) != len(val_idx)` (2156)!\n"
     ]
    }
   ],
   "source": [
    "num_test = len(new_dataset)\n",
    "custom_data_test = spk.data.AtomsDataModule(\n",
    "    \"./qm8_test_schnet.db\",\n",
    "    batch_size=10,\n",
    "    distance_unit=\"Ang\",\n",
    "    property_units={\n",
    "        \"E1_CC2\": \"Hartree\",\n",
    "        # \"E2_CC2\": \"Hartree\",\n",
    "    },\n",
    "    num_train=0.01,\n",
    "    num_val=0.99,\n",
    "    num_test=0.0,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.0),\n",
    "        # trn.RemoveOffsets(QM9.U0, remove_mean=True, remove_atomrefs=True),\n",
    "        trn.CastTo32(),\n",
    "    ],\n",
    "    num_workers=1,\n",
    "    split_file=\"./qm8_split_test.npz\",\n",
    "    pin_memory=True,  # set to false, when not using a GPU\n",
    "    load_properties=[\"E1_CC2\"],\n",
    ")\n",
    "custom_data_test.prepare_data()\n",
    "custom_data_test.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 216/216 [00:02<00:00, 79.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/schnet/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_E1_CC2_MAE       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.020474446937441826    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_E1_CC2_r2       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6275290250778198     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0006873265374451876   </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_E1_CC2_MAE      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.020474446937441826   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_E1_CC2_r2      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6275290250778198    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0006873265374451876  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.0006873265374451876,\n",
       "  'val_E1_CC2_MAE': 0.020474446937441826,\n",
       "  'val_E1_CC2_r2': 0.6275290250778198}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom_data_test\n",
    "trainer.validate(task, datamodule=custom_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(os.path.join(qm9tut, \"best_inference_model\"))\n",
    "best_model.cpu()\n",
    "\n",
    "for batch in qm9data.test_dataloader():\n",
    "    # result = nnpot(batch)\n",
    "\n",
    "    targets = {\n",
    "        output.target_property: batch[output.target_property]\n",
    "        for output in task.outputs\n",
    "        if not isinstance(output, UnsupervisedModelOutput)\n",
    "    }\n",
    "    try:\n",
    "        targets[\"considered_atoms\"] = batch[\"considered_atoms\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    pred = task.predict_without_postprocessing(batch)\n",
    "    pred, targets = task.apply_constraints(pred, targets)\n",
    "    target_dict[\"U0\"].append(pred[\"energy_U0\"].detach().numpy())\n",
    "    pred_dict[\"U0\"].append(targets[\"energy_U0\"].detach().numpy())\n",
    "    print(\"Result dictionary:\", pred)\n",
    "    print(\"Target dictionary:\", targets)\n",
    "    # print(\"Result dictionary:\", result)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
