{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline GNN model for node-level regression\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from torchmetrics.wrappers import MultioutputWrapper\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "\n",
    "from qtaim_embed.models.graph_level.base_gcn import GCNGraphPred\n",
    "from qtaim_embed.utils.data import get_default_graph_level_config\n",
    "from qtaim_embed.models.layers import (\n",
    "    GraphConvDropoutBatch,\n",
    "    ResidualBlock,\n",
    "    UnifySize,\n",
    "    Set2SetThenCat,\n",
    "    SumPoolingThenCat,\n",
    "    WeightAndSumThenCat,\n",
    "    GlobalAttentionPoolingThenCat,\n",
    ")\n",
    "\n",
    "from qtaim_embed.utils.models import (\n",
    "    get_layer_args,\n",
    "    link_fmt_to_node_fmt,\n",
    "    _split_batched_output,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_layer_args(hparams, layer_ind=None, embedding_in=False, activation=None):\\n\\n\\n    assert hparams.conv_fn in [\\n        \"GraphConvDropoutBatch\",\\n        \"ResidualBlock\",\\n        \"GATConv\",\\n    ], \"conv_fn must be either GraphConvDropoutBatch or ResidualBlock\"\\n\\n    layer_args = {}\\n    if hparams.conv_fn == \"GraphConvDropoutBatch\":\\n        atom_out = hparams.atom_input_size\\n        bond_out = hparams.bond_input_size\\n        global_out = hparams.global_input_size\\n        atom_in = hparams.atom_input_size\\n        bond_in = hparams.bond_input_size\\n        global_in = hparams.global_input_size\\n\\n        if layer_ind == hparams.n_conv_layers - 1:\\n            if \"atom\" in hparams.target_dict.keys():\\n                atom_out = len(hparams.target_dict[\"atom\"])\\n            if \"bond\" in hparams.target_dict.keys():\\n                bond_out = len(hparams.target_dict[\"bond\"])\\n            if \"global\" in hparams.target_dict.keys():\\n                global_out = len(hparams.target_dict[\"global\"])\\n\\n        if embedding_in:\\n            atom_in = hparams.embedding_size\\n            bond_in = hparams.embedding_size\\n            global_in = hparams.embedding_size\\n            atom_out = hparams.embedding_size\\n            bond_out = hparams.embedding_size\\n            global_out = hparams.embedding_size\\n\\n        layer_args[\"a2b\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": bond_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"b2a\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": atom_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"a2g\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": global_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"b2g\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": global_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"g2a\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": atom_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"g2b\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": bond_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"a2a\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": atom_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"b2b\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": bond_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"g2g\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": global_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n\\n    elif hparams.conv_fn == \"ResidualBlock\":\\n        atom_out = hparams.atom_input_size\\n        bond_out = hparams.bond_input_size\\n        global_out = hparams.global_input_size\\n        atom_in = hparams.atom_input_size\\n        bond_in = hparams.bond_input_size\\n        global_in = hparams.global_input_size\\n\\n        if embedding_in:\\n            atom_in = hparams.embedding_size\\n            bond_in = hparams.embedding_size\\n            global_in = hparams.embedding_size\\n            atom_out = hparams.embedding_size\\n            bond_out = hparams.embedding_size\\n            global_out = hparams.embedding_size\\n\\n        # resid_n_graph_convs = hparams.resid_n_graph_convs\\n\\n        if layer_ind != -1:  # last residual layer has different args\\n            # print(\"triggered early stop condition!!!\")\\n            layer_args[\"a2b_inner\"] = {\\n                \"in_feats\": atom_in,\\n                \"out_feats\": bond_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"b2a_inner\"] = {\\n                \"in_feats\": bond_in,\\n                \"out_feats\": atom_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"a2g_inner\"] = {\\n                \"in_feats\": atom_in,\\n                \"out_feats\": global_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"b2g_inner\"] = {\\n                \"in_feats\": bond_in,\\n                \"out_feats\": global_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"g2a_inner\"] = {\\n                \"in_feats\": global_in,\\n                \"out_feats\": atom_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"g2b_inner\"] = {\\n                \"in_feats\": global_in,\\n                \"out_feats\": bond_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"a2a_inner\"] = {\\n                \"in_feats\": atom_in,\\n                \"out_feats\": atom_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"b2b_inner\"] = {\\n                \"in_feats\": bond_in,\\n                \"out_feats\": bond_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            layer_args[\"g2g_inner\"] = {\\n                \"in_feats\": global_in,\\n                \"out_feats\": global_out,\\n                \"norm\": hparams.norm,\\n                \"bias\": hparams.bias,\\n                \"activation\": activation,\\n                \"allow_zero_in_degree\": True,\\n                \"dropout\": hparams.dropout,\\n                \"batch_norm_tf\": hparams.batch_norm_tf,\\n            }\\n\\n            if \"atom\" in hparams.target_dict.keys():\\n                atom_out = len(hparams.target_dict[\"atom\"])\\n            if \"bond\" in hparams.target_dict.keys():\\n                bond_out = len(hparams.target_dict[\"bond\"])\\n            if \"global\" in hparams.target_dict.keys():\\n                global_out = len(hparams.target_dict[\"global\"])\\n            # print(\"target_dict\", hparams.target_dict)\\n\\n        layer_args[\"a2b\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": bond_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"b2a\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": atom_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"a2g\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": global_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"b2g\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": global_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"g2a\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": atom_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"g2b\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": bond_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"a2a\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": atom_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"b2b\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": bond_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n        layer_args[\"g2g\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": global_out,\\n            \"norm\": hparams.norm,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n            \"allow_zero_in_degree\": True,\\n            \"dropout\": hparams.dropout,\\n            \"batch_norm_tf\": hparams.batch_norm_tf,\\n        }\\n\\n    \\n    elif hparams.conv_fn == \"GATConv\":\\n    \\n        atom_out = hparams.hidden_size\\n        bond_out = hparams.hidden_size\\n        global_out = hparams.hidden_size\\n        atom_in = hparams.atom_input_size\\n        bond_in = hparams.bond_input_size\\n        global_in = hparams.global_input_size\\n        num_heads = hparams.num_heads\\n\\n        if layer_ind > 0: \\n            atom_in = hparams.hidden_size \\n            bond_in = hparams.hidden_size\\n            global_in = hparams.hidden_size \\n\\n            if num_heads > 1: \\n                atom_in = hparams.hidden_size * num_heads\\n                bond_in = hparams.hidden_size * num_heads \\n                global_in = hparams.hidden_size * num_heads \\n        else: \\n            if embedding_in:\\n                atom_in = hparams.embedding_size\\n                bond_in = hparams.embedding_size\\n                global_in = hparams.embedding_size\\n\\n        if layer_ind == hparams.n_conv_layers - 1:\\n            num_heads = 1\\n            \\n\\n            #atom_out = hparams.embedding_size\\n            #bond_out = hparams.embedding_size\\n            #global_out = hparams.embedding_size\\n\\n        layer_args[\"a2b\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": bond_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"b2a\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": atom_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"a2g\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": global_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"b2g\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": global_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"g2a\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": atom_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"g2b\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": bond_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"a2a\"] = {\\n            \"in_feats\": atom_in,\\n            \"out_feats\": atom_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"b2b\"] = {\\n            \"in_feats\": bond_in,\\n            \"out_feats\": bond_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n\\n        layer_args[\"g2g\"] = {\\n            \"in_feats\": global_in,\\n            \"out_feats\": global_out,\\n            \"num_heads\": num_heads,\\n            \"feat_drop\": hparams.feat_drop,\\n            \"attn_drop\": hparams.attn_drop,\\n            \"residual\": hparams.residual,\\n            \"allow_zero_in_degree\": True,\\n            \"bias\": hparams.bias,\\n            \"activation\": activation,\\n        }\\n        \\n\\n    return layer_args\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_layer_args(hparams, layer_ind=None, embedding_in=False, activation=None):\n",
    "\n",
    "\n",
    "    assert hparams.conv_fn in [\n",
    "        \"GraphConvDropoutBatch\",\n",
    "        \"ResidualBlock\",\n",
    "        \"GATConv\",\n",
    "    ], \"conv_fn must be either GraphConvDropoutBatch or ResidualBlock\"\n",
    "\n",
    "    layer_args = {}\n",
    "    if hparams.conv_fn == \"GraphConvDropoutBatch\":\n",
    "        atom_out = hparams.atom_input_size\n",
    "        bond_out = hparams.bond_input_size\n",
    "        global_out = hparams.global_input_size\n",
    "        atom_in = hparams.atom_input_size\n",
    "        bond_in = hparams.bond_input_size\n",
    "        global_in = hparams.global_input_size\n",
    "\n",
    "        if layer_ind == hparams.n_conv_layers - 1:\n",
    "            if \"atom\" in hparams.target_dict.keys():\n",
    "                atom_out = len(hparams.target_dict[\"atom\"])\n",
    "            if \"bond\" in hparams.target_dict.keys():\n",
    "                bond_out = len(hparams.target_dict[\"bond\"])\n",
    "            if \"global\" in hparams.target_dict.keys():\n",
    "                global_out = len(hparams.target_dict[\"global\"])\n",
    "\n",
    "        if embedding_in:\n",
    "            atom_in = hparams.embedding_size\n",
    "            bond_in = hparams.embedding_size\n",
    "            global_in = hparams.embedding_size\n",
    "            atom_out = hparams.embedding_size\n",
    "            bond_out = hparams.embedding_size\n",
    "            global_out = hparams.embedding_size\n",
    "\n",
    "        layer_args[\"a2b\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2a\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"a2g\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2g\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2a\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2b\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"a2a\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2b\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2g\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "\n",
    "    elif hparams.conv_fn == \"ResidualBlock\":\n",
    "        atom_out = hparams.atom_input_size\n",
    "        bond_out = hparams.bond_input_size\n",
    "        global_out = hparams.global_input_size\n",
    "        atom_in = hparams.atom_input_size\n",
    "        bond_in = hparams.bond_input_size\n",
    "        global_in = hparams.global_input_size\n",
    "\n",
    "        if embedding_in:\n",
    "            atom_in = hparams.embedding_size\n",
    "            bond_in = hparams.embedding_size\n",
    "            global_in = hparams.embedding_size\n",
    "            atom_out = hparams.embedding_size\n",
    "            bond_out = hparams.embedding_size\n",
    "            global_out = hparams.embedding_size\n",
    "\n",
    "        # resid_n_graph_convs = hparams.resid_n_graph_convs\n",
    "\n",
    "        if layer_ind != -1:  # last residual layer has different args\n",
    "            # print(\"triggered early stop condition!!!\")\n",
    "            layer_args[\"a2b_inner\"] = {\n",
    "                \"in_feats\": atom_in,\n",
    "                \"out_feats\": bond_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"b2a_inner\"] = {\n",
    "                \"in_feats\": bond_in,\n",
    "                \"out_feats\": atom_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"a2g_inner\"] = {\n",
    "                \"in_feats\": atom_in,\n",
    "                \"out_feats\": global_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"b2g_inner\"] = {\n",
    "                \"in_feats\": bond_in,\n",
    "                \"out_feats\": global_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"g2a_inner\"] = {\n",
    "                \"in_feats\": global_in,\n",
    "                \"out_feats\": atom_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"g2b_inner\"] = {\n",
    "                \"in_feats\": global_in,\n",
    "                \"out_feats\": bond_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"a2a_inner\"] = {\n",
    "                \"in_feats\": atom_in,\n",
    "                \"out_feats\": atom_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"b2b_inner\"] = {\n",
    "                \"in_feats\": bond_in,\n",
    "                \"out_feats\": bond_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            layer_args[\"g2g_inner\"] = {\n",
    "                \"in_feats\": global_in,\n",
    "                \"out_feats\": global_out,\n",
    "                \"norm\": hparams.norm,\n",
    "                \"bias\": hparams.bias,\n",
    "                \"activation\": activation,\n",
    "                \"allow_zero_in_degree\": True,\n",
    "                \"dropout\": hparams.dropout,\n",
    "                \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "            }\n",
    "\n",
    "            if \"atom\" in hparams.target_dict.keys():\n",
    "                atom_out = len(hparams.target_dict[\"atom\"])\n",
    "            if \"bond\" in hparams.target_dict.keys():\n",
    "                bond_out = len(hparams.target_dict[\"bond\"])\n",
    "            if \"global\" in hparams.target_dict.keys():\n",
    "                global_out = len(hparams.target_dict[\"global\"])\n",
    "            # print(\"target_dict\", hparams.target_dict)\n",
    "\n",
    "        layer_args[\"a2b\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2a\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"a2g\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2g\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2a\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2b\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"a2a\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2b\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2g\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"norm\": hparams.norm,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"dropout\": hparams.dropout,\n",
    "            \"batch_norm_tf\": hparams.batch_norm_tf,\n",
    "        }\n",
    "\n",
    "    \n",
    "    elif hparams.conv_fn == \"GATConv\":\n",
    "    \n",
    "        atom_out = hparams.hidden_size\n",
    "        bond_out = hparams.hidden_size\n",
    "        global_out = hparams.hidden_size\n",
    "        atom_in = hparams.atom_input_size\n",
    "        bond_in = hparams.bond_input_size\n",
    "        global_in = hparams.global_input_size\n",
    "        num_heads = hparams.num_heads\n",
    "\n",
    "        if layer_ind > 0: \n",
    "            atom_in = hparams.hidden_size \n",
    "            bond_in = hparams.hidden_size\n",
    "            global_in = hparams.hidden_size \n",
    "\n",
    "            if num_heads > 1: \n",
    "                atom_in = hparams.hidden_size * num_heads\n",
    "                bond_in = hparams.hidden_size * num_heads \n",
    "                global_in = hparams.hidden_size * num_heads \n",
    "        else: \n",
    "            if embedding_in:\n",
    "                atom_in = hparams.embedding_size\n",
    "                bond_in = hparams.embedding_size\n",
    "                global_in = hparams.embedding_size\n",
    "\n",
    "        if layer_ind == hparams.n_conv_layers - 1:\n",
    "            num_heads = 1\n",
    "            \n",
    "\n",
    "            #atom_out = hparams.embedding_size\n",
    "            #bond_out = hparams.embedding_size\n",
    "            #global_out = hparams.embedding_size\n",
    "\n",
    "        layer_args[\"a2b\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2a\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"a2g\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2g\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2a\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2b\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"a2a\"] = {\n",
    "            \"in_feats\": atom_in,\n",
    "            \"out_feats\": atom_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"b2b\"] = {\n",
    "            \"in_feats\": bond_in,\n",
    "            \"out_feats\": bond_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "\n",
    "        layer_args[\"g2g\"] = {\n",
    "            \"in_feats\": global_in,\n",
    "            \"out_feats\": global_out,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"feat_drop\": hparams.feat_drop,\n",
    "            \"attn_drop\": hparams.attn_drop,\n",
    "            \"residual\": hparams.residual,\n",
    "            \"allow_zero_in_degree\": True,\n",
    "            \"bias\": hparams.bias,\n",
    "            \"activation\": activation,\n",
    "        }\n",
    "        \n",
    "\n",
    "    return layer_args\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (517551615.py, line 347)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 347\u001b[0;36m\u001b[0m\n\u001b[0;31m    Forward pass\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class GCNGraphPred(pl.LightningModule):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        atom_input_size=12,\n",
    "        bond_input_size=8,\n",
    "        global_input_size=3,\n",
    "        n_conv_layers=3,\n",
    "        target_dict={\"atom\": \"E\"},\n",
    "        conv_fn=\"GraphConvDropoutBatch\",\n",
    "        global_pooling=\"WeightAndSumThenCat\",\n",
    "        resid_n_graph_convs=None,\n",
    "        num_heads_gat=2, \n",
    "        dropout_feat_gat=0.2, \n",
    "        dropout_attn_gat=0.2,\n",
    "        hidden_size_gat=128,\n",
    "        residual_gat=True,\n",
    "        dropout=0.2,\n",
    "        batch_norm=True,\n",
    "        activation=\"ReLU\",\n",
    "        bias=True,\n",
    "        norm=\"both\",\n",
    "        aggregate=\"sum\",\n",
    "        lr=1e-3,\n",
    "        scheduler_name=\"reduce_on_plateau\",\n",
    "        weight_decay=0.0,\n",
    "        lr_plateau_patience=5,\n",
    "        lr_scale_factor=0.5,\n",
    "        loss_fn=\"mse\",\n",
    "        embedding_size=128,\n",
    "        fc_layer_size=[128, 64],\n",
    "        fc_dropout=0.0,\n",
    "        fc_batch_norm=True,\n",
    "        lstm_iters=3,\n",
    "        lstm_layers=1,\n",
    "        pooling_ntypes=[\"atom\", \"bond\"],\n",
    "        pooling_ntypes_direct=[\"global\"],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        # output_dims = 0\n",
    "        # for k, v in target_dict.items():\n",
    "        #    output_dims += len(v)\n",
    "\n",
    "        assert conv_fn == \"GraphConvDropoutBatch\" or conv_fn == \"ResidualBlock\" or conv_fn == \"GATConv\", (\n",
    "            \"conv_fn must be either GraphConvDropoutBatch, GATConvDropoutBatch or ResidualBlock\"\n",
    "            + f\"but got {conv_fn}\"\n",
    "        )\n",
    "\n",
    "        if conv_fn == \"ResidualBlock\":\n",
    "            assert resid_n_graph_convs is not None, (\n",
    "                \"resid_n_graph_convs must be specified for ResidualBlock\"\n",
    "                + f\"but got {resid_n_graph_convs}\"\n",
    "            )\n",
    "\n",
    "        assert global_pooling in [\n",
    "            \"WeightAndSumThenCat\",\n",
    "            \"SumPoolingThenCat\",\n",
    "            \"GlobalAttentionPoolingThenCat\",\n",
    "            \"Set2SetThenCat\",\n",
    "        ], (\n",
    "            \"global_pooling must be either WeightAndSumThenCat, SumPoolingThenCat, or GlobalAttentionPoolingThenCat\"\n",
    "            + f\"but got {global_pooling}\"\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            \"atom_input_size\": atom_input_size,\n",
    "            \"bond_input_size\": bond_input_size,\n",
    "            \"global_input_size\": global_input_size,\n",
    "            \"conv_fn\": conv_fn,\n",
    "            \"target_dict\": target_dict,\n",
    "            \"dropout\": dropout,\n",
    "            \"batch_norm_tf\": batch_norm,\n",
    "            \"activation\": activation,\n",
    "            \"bias\": bias,\n",
    "            \"norm\": norm,\n",
    "            \"aggregate\": aggregate,\n",
    "            \"n_conv_layers\": n_conv_layers,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lr_plateau_patience\": lr_plateau_patience,\n",
    "            \"lr_scale_factor\": lr_scale_factor,\n",
    "            \"scheduler_name\": scheduler_name,\n",
    "            \"loss_fn\": loss_fn,\n",
    "            \"resid_n_graph_convs\": resid_n_graph_convs,\n",
    "            \"embedding_size\": embedding_size,\n",
    "            \"fc_layer_size\": fc_layer_size,\n",
    "            \"fc_dropout\": fc_dropout,\n",
    "            \"fc_batch_norm\": fc_batch_norm,\n",
    "            \"n_fc_layers\": len(fc_layer_size),\n",
    "            \"global_pooling\": global_pooling,\n",
    "            \"ntypes_pool\": pooling_ntypes,\n",
    "            \"ntypes_pool_direct_cat\": pooling_ntypes_direct,\n",
    "            \"lstm_iters\": lstm_iters,\n",
    "            \"lstm_layers\": lstm_layers,\n",
    "            \"num_heads\": num_heads_gat,\n",
    "            \"feat_drop\": dropout_feat_gat,\n",
    "            \"attn_drop\": dropout_attn_gat,\n",
    "            \"residual\": residual_gat,\n",
    "            \"hidden_size\": hidden_size_gat,\n",
    "            \"ntasks\": len(target_dict[\"global\"]),\n",
    "        }\n",
    "\n",
    "        self.hparams.update(params)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # convert string activation to function\n",
    "        if self.hparams.activation is not None:\n",
    "            self.activation = getattr(torch.nn, self.hparams.activation)()\n",
    "        else: \n",
    "            self.activation = None\n",
    "\n",
    "        input_size = {\n",
    "            \"atom\": self.hparams.atom_input_size,\n",
    "            \"bond\": self.hparams.bond_input_size,\n",
    "            \"global\": self.hparams.global_input_size,\n",
    "        }\n",
    "        # print(\"input size\", input_size)\n",
    "        self.embedding = UnifySize(\n",
    "            input_dim=input_size,\n",
    "            output_dim=self.hparams.embedding_size,\n",
    "        )\n",
    "        # self.embedding_output_size = self.hparams.embedding_size\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        if self.hparams.conv_fn == \"GraphConvDropoutBatch\":\n",
    "            for i in range(self.hparams.n_conv_layers):\n",
    "                # embedding_in = False\n",
    "                # if i == 0:\n",
    "                embedding_in = True\n",
    "\n",
    "                layer_args = get_layer_args(self.hparams, i, activation=self.activation, embedding_in=embedding_in)\n",
    "                # print(\"resid layer args\", layer_args)\n",
    "\n",
    "                self.conv_layers.append(\n",
    "                    dglnn.HeteroGraphConv(\n",
    "                        {\n",
    "                            \"a2b\": GraphConvDropoutBatch(**layer_args[\"a2b\"]),\n",
    "                            \"b2a\": GraphConvDropoutBatch(**layer_args[\"b2a\"]),\n",
    "                            \"a2g\": GraphConvDropoutBatch(**layer_args[\"a2g\"]),\n",
    "                            \"g2a\": GraphConvDropoutBatch(**layer_args[\"g2a\"]),\n",
    "                            \"b2g\": GraphConvDropoutBatch(**layer_args[\"b2g\"]),\n",
    "                            \"g2b\": GraphConvDropoutBatch(**layer_args[\"g2b\"]),\n",
    "                            \"a2a\": GraphConvDropoutBatch(**layer_args[\"a2a\"]),\n",
    "                            \"b2b\": GraphConvDropoutBatch(**layer_args[\"b2b\"]),\n",
    "                            \"g2g\": GraphConvDropoutBatch(**layer_args[\"g2g\"]),\n",
    "                        },\n",
    "                        aggregate=self.hparams.aggregate,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        elif self.hparams.conv_fn == \"ResidualBlock\":\n",
    "            layer_tracker = 0\n",
    "            embedding_in = True\n",
    "\n",
    "            while layer_tracker < self.hparams.n_conv_layers:\n",
    "                if (\n",
    "                    layer_tracker + self.hparams.resid_n_graph_convs\n",
    "                    > self.hparams.n_conv_layers - 1\n",
    "                ):\n",
    "                    # print(\"triggered output_layer args\")\n",
    "                    layer_ind = self.hparams.n_conv_layers - layer_tracker - 1\n",
    "                else:\n",
    "                    layer_ind = -1\n",
    "\n",
    "                layer_args = get_layer_args(\n",
    "                    self.hparams, layer_ind, embedding_in=embedding_in, activation=self.activation\n",
    "                )\n",
    "                # print(\"resid layer args\", layer_args)\n",
    "                # for k, v in layer_args.items():\n",
    "                #    print(k, v[\"in_feats\"], v[\"out_feats\"])\n",
    "\n",
    "                # embedding_in = False\n",
    "                output_block = False\n",
    "                if layer_ind != -1:\n",
    "                    output_block = True\n",
    "\n",
    "                self.conv_layers.append(\n",
    "                    ResidualBlock(\n",
    "                        layer_args,\n",
    "                        resid_n_graph_convs=self.hparams.resid_n_graph_convs,\n",
    "                        aggregate=self.hparams.aggregate,\n",
    "                        output_block=output_block,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                layer_tracker += self.hparams.resid_n_graph_convs\n",
    "\n",
    "        elif self.hparams.conv_fn == \"GATConv\":\n",
    "            for i in range(self.hparams.n_conv_layers):\n",
    "                # embedding_in = False\n",
    "                # if i == 0:\n",
    "                embedding_in = True\n",
    "\n",
    "                layer_args = get_layer_args(self.hparams, i, activation=self.activation, embedding_in=True)\n",
    "                # print(\"resid layer args\", layer_args)\n",
    "\n",
    "                self.conv_layers.append(\n",
    "                    dglnn.HeteroGraphConv(\n",
    "                        {\n",
    "                            \"a2b\": GATConv(**layer_args[\"a2b\"]),\n",
    "                            \"b2a\": GATConv(**layer_args[\"b2a\"]),\n",
    "                            \"a2g\": GATConv(**layer_args[\"a2g\"]),\n",
    "                            \"g2a\": GATConv(**layer_args[\"g2a\"]),\n",
    "                            \"b2g\": GATConv(**layer_args[\"b2g\"]),\n",
    "                            \"g2b\": GATConv(**layer_args[\"g2b\"]),\n",
    "                            \"a2a\": GATConv(**layer_args[\"a2a\"]),\n",
    "                            \"b2b\": GATConv(**layer_args[\"b2b\"]),\n",
    "                            \"g2g\": GATConv(**layer_args[\"g2g\"]),\n",
    "                        },\n",
    "                        aggregate=self.hparams.aggregate,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "        self.conv_layers = nn.ModuleList(self.conv_layers)\n",
    "        # print(\"conv layer out modes\", self.conv_layers[-1].mods)\n",
    "\n",
    "        # print(\"conv layer out feats\", self.conv_layers[-1].out_feats)\n",
    "        # conv_out_size = self.conv_layers[-1].out_feats\n",
    "\n",
    "        if self.hparams.conv_fn == \"GraphConvDropoutBatch\":\n",
    "            conv_out_size = {}\n",
    "            for k, v in self.conv_layers[-1].mods.items():\n",
    "                conv_out_size[k] = v.out_feats\n",
    "\n",
    "        elif self.hparams.conv_fn == \"ResidualBlock\":\n",
    "            conv_out_size = self.conv_layers[-1].out_feats\n",
    "        \n",
    "        elif self.hparams.conv_fn == \"GATConv\":\n",
    "            conv_out_size = {}\n",
    "            for k, v in self.conv_layers[-1].mods.items():\n",
    "                conv_out_size[k] = v._out_feats\n",
    "        \n",
    "        \n",
    "        self.conv_out_size = link_fmt_to_node_fmt(conv_out_size)\n",
    "\n",
    "\n",
    "        ####################### readout starts here ######################\n",
    "        if self.hparams.global_pooling == \"WeightAndSumThenCat\":\n",
    "            readout_fn = WeightAndSumThenCat\n",
    "        elif self.hparams.global_pooling == \"SumPoolingThenCat\":\n",
    "            readout_fn = SumPoolingThenCat\n",
    "        elif self.hparams.global_pooling == \"GlobalAttentionPoolingThenCat\":\n",
    "            readout_fn = GlobalAttentionPoolingThenCat\n",
    "        elif self.hparams.global_pooling == \"Set2SetThenCat\":\n",
    "            readout_fn = Set2SetThenCat\n",
    "\n",
    "        list_in_feats = []\n",
    "        for type_feat in self.hparams.pooling_ntypes:\n",
    "            list_in_feats.append(self.conv_out_size[type_feat])\n",
    "\n",
    "        self.readout_out_size = 0\n",
    "\n",
    "        if self.hparams.global_pooling == \"Set2SetThenCat\":\n",
    "\n",
    "            self.readout = readout_fn(\n",
    "                n_iters=self.hparams.lstm_iters,\n",
    "                n_layers=self.hparams.lstm_layers,\n",
    "                in_feats=list_in_feats,\n",
    "                ntypes=self.hparams.pooling_ntypes,\n",
    "                ntypes_direct_cat=self.hparams.ntypes_pool_direct_cat,\n",
    "            )\n",
    "            for i in self.hparams.pooling_ntypes:\n",
    "                if i not in self.hparams.ntypes_pool_direct_cat:\n",
    "                    self.readout_out_size += self.conv_out_size[i] * 2\n",
    "                else:\n",
    "                    self.readout_out_size += self.conv_out_size[i]\n",
    "\n",
    "        else:\n",
    "            # print(\"other readout used\")\n",
    "            self.readout = readout_fn(\n",
    "                ntypes=self.hparams.pooling_ntypes,\n",
    "                in_feats=list_in_feats,\n",
    "                ntypes_direct_cat=self.hparams.ntypes_pool_direct_cat,\n",
    "            )\n",
    "\n",
    "            for i in self.hparams.pooling_ntypes:\n",
    "                if i in self.hparams.ntypes_pool_direct_cat:\n",
    "                    self.readout_out_size += self.conv_out_size[i]\n",
    "                else:\n",
    "                    self.readout_out_size += self.conv_out_size[i]\n",
    "        #if self.hparams.conv_fn == \"GATConv\":\n",
    "        #self.readout_out_size = self.hparams.hidden_size * self.hparams.num_heads\n",
    "        # print(\"readout out size\", self.readout_out_size)\n",
    "        # self.readout_out_size = readout_out_size\n",
    "        self.loss = self.loss_function()\n",
    "        ####################### fc starts here ######################\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        input_size = self.readout_out_size\n",
    "        print(\"readout in size\", input_size)\n",
    "        for i in range(self.hparams.n_fc_layers):\n",
    "            out_size = self.hparams.fc_layer_size[i]\n",
    "            self.fc_layers.append(nn.Linear(input_size, out_size))\n",
    "            if self.hparams.fc_batch_norm:\n",
    "                self.fc_layers.append(nn.BatchNorm1d(out_size))\n",
    "            if self.activation is not None:\n",
    "                self.fc_layers.append(self.activation)\n",
    "            if self.hparams.fc_dropout > 0:\n",
    "                self.fc_layers.append(nn.Dropout(self.hparams.fc_dropout))\n",
    "            input_size = out_size\n",
    "\n",
    "        self.fc_layers.append(nn.Linear(input_size, self.hparams.ntasks))\n",
    "\n",
    "        # print(\"number of output dims\", output_dims)\n",
    "        print(\"... > number of tasks:\", self.hparams.ntasks)\n",
    "\n",
    "        # create multioutput wrapper for metrics\n",
    "        self.train_r2 = MultioutputWrapper(\n",
    "            torchmetrics.R2Score(), num_outputs=self.hparams.ntasks\n",
    "        )\n",
    "        self.train_torch_l1 = MultioutputWrapper(\n",
    "            torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.ntasks\n",
    "        )\n",
    "        self.train_torch_mse = MultioutputWrapper(\n",
    "            torchmetrics.MeanSquaredError(squared=False),\n",
    "            num_outputs=self.hparams.ntasks,\n",
    "        )\n",
    "        self.val_r2 = MultioutputWrapper(\n",
    "            torchmetrics.R2Score(), num_outputs=self.hparams.ntasks\n",
    "        )\n",
    "        self.val_torch_l1 = MultioutputWrapper(\n",
    "            torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.ntasks\n",
    "        )\n",
    "        self.val_torch_mse = MultioutputWrapper(\n",
    "            torchmetrics.MeanSquaredError(squared=False),\n",
    "            num_outputs=self.hparams.ntasks,\n",
    "        )\n",
    "        self.test_r2 = MultioutputWrapper(\n",
    "            torchmetrics.R2Score(), num_outputs=self.hparams.ntasks\n",
    "        )\n",
    "        self.test_torch_l1 = MultioutputWrapper(\n",
    "            torchmetrics.MeanAbsoluteError(), num_outputs=self.hparams.ntasks\n",
    "        )\n",
    "        self.test_torch_mse = MultioutputWrapper(\n",
    "            torchmetrics.MeanSquaredError(squared=False),\n",
    "            num_outputs=self.hparams.ntasks,\n",
    "        )\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        feats = self.embedding(inputs)\n",
    "        for ind, conv in enumerate(self.conv_layers):\n",
    "            feats = conv(graph, feats)\n",
    "            \n",
    "            if self.hparams.conv_fn == \"GATConv\":\n",
    "                if ind < self.hparams.n_conv_layers - 1:\n",
    "                    for k, v in feats.items():\n",
    "                        feats[k] = v.reshape(-1, self.hparams.num_heads * self.hparams.hidden_size)\n",
    "                else:         \n",
    "                    for k, v in feats.items():\n",
    "                        feats[k] = v.reshape(-1, self.hparams.hidden_size)\n",
    "\n",
    "        readout_feats = self.readout(graph, feats)\n",
    "        for ind, layer in enumerate(self.fc_layers):\n",
    "            readout_feats = layer(readout_feats)\n",
    "\n",
    "        #print(\"preds shape:\", readout_feats.shape)\n",
    "        return readout_feats\n",
    "    \n",
    "    def loss_function(self):\n",
    "\n",
    "        if self.hparams.ntasks > 1:\n",
    "            loss_fn = nn.ModuleList()\n",
    "            for i in range(self.hparams.ntasks):\n",
    "                if self.hparams.loss_fn == \"mse\":\n",
    "                    loss_fn.append(torchmetrics.MeanSquaredError())\n",
    "                elif self.hparams.loss_fn == \"smape\":\n",
    "                    loss_fn.append(torchmetrics.SymmetricMeanAbsolutePercentageError())\n",
    "                elif self.hparams.loss_fn == \"mae\":\n",
    "                    loss_fn.append(torchmetrics.MeanAbsoluteError())\n",
    "                else:\n",
    "                    loss_fn.append(torchmetrics.MeanSquaredError())\n",
    "\n",
    "        else: \n",
    "            if self.hparams.loss_fn == \"mse\":\n",
    "                loss_fn = torchmetrics.MeanSquaredError()\n",
    "                \n",
    "            elif self.hparams.loss_fn == \"smape\":\n",
    "                loss_fn = torchmetrics.SymmetricMeanAbsolutePercentageError()\n",
    "            elif self.hparams.loss_fn == \"mae\":\n",
    "                loss_fn = torchmetrics.MeanAbsoluteError()\n",
    "            else:\n",
    "                loss_fn = torchmetrics.MeanSquaredError()\n",
    "\n",
    "        return loss_fn\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qtaim_embed.utils.grapher import get_grapher\n",
    "# from qtaim_embed.data.molwrapper import mol_wrappers_from_df\n",
    "\n",
    "from qtaim_embed.core.dataset import HeteroGraphGraphLabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_default_graph_level_config()\n",
    "config[\"log_scale_features\"] = True\n",
    "config[\"log_scale_targets\"] = False\n",
    "config[\"standard_scale_features\"] = True\n",
    "config[\"standard_scale_targets\"] = True\n",
    "config[\"debug\"] = False\n",
    "config[\n",
    "    \"train_dataset_loc\"\n",
    "] = \"/home/santiagovargas/dev/qtaim_embed/data/xyz_qm8/molecules_qtaim_labelled.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > creating MoleculeWrapper objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 5931.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > bond_feats_error_count:  0\n",
      "... > atom_feats_error_count:  0\n",
      "element set {'N', 'C', 'H', 'O'}\n",
      "selected atomic keys ['extra_feat_atom_esp_total']\n",
      "selected bond keys ['extra_feat_bond_esp_total', 'bond_length']\n",
      "selected global keys ['extra_feat_global_E1_CAM']\n",
      "... > Building graphs and featurizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 309.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included in labels\n",
      "{'global': ['extra_feat_global_E1_CAM']}\n",
      "included in graph features\n",
      "{'atom': ['total_degree', 'total_H', 'is_in_ring', 'ring_size_3', 'ring_size_4', 'ring_size_5', 'ring_size_6', 'ring_size_7', 'chemical_symbol_N', 'chemical_symbol_C', 'chemical_symbol_H', 'chemical_symbol_O', 'extra_feat_atom_esp_total'], 'bond': ['metal bond', 'ring inclusion', 'ring size_3', 'ring size_4', 'ring size_5', 'ring size_6', 'ring size_7', 'bond_length', 'extra_feat_bond_esp_total'], 'global': ['num atoms', 'num bonds', 'molecule weight']}\n",
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys([])\n",
      "include names:  dict_keys(['global'])\n",
      "... > parsing labels and features in graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 37286.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys(['global'])\n",
      "... > Log scaling features\n",
      "... > Log scaling features complete\n",
      "... > Scaling features\n",
      "mean [1.03348744 0.29579238 0.1931437  0.02300363 0.03993083 0.06814284\n",
      " 0.04296905 0.01909736 0.04557323 0.23654678 0.35547122 0.05555594\n",
      " 9.03963532]\n",
      "std [0.3914866  0.46054609 0.3107612  0.12416012 0.16150379 0.2063724\n",
      " 0.16714525 0.11345734 0.17179068 0.32864473 0.34645936 0.18820728\n",
      " 5.66985947]\n",
      "mean [0.         0.19579709 0.02420918 0.04417114 0.07007922 0.04587003\n",
      " 0.0208114  0.81517279 0.66599706]\n",
      "std [0.         0.31205721 0.12725739 0.16931041 0.2089596  0.17230968\n",
      " 0.11828885 0.09367724 0.18940919]\n",
      "Standard deviation for feature 0 is 0.0, smaller than 0.001. You may want to exclude this feature.\n",
      "mean [2.81851833 2.8375313  4.69764359]\n",
      "std [0.16318695 0.17198598 0.06353343]\n",
      "... > Scaling features complete\n",
      "... > feature mean(s): \n",
      " {'atom': tensor([1.0335, 0.2958, 0.1931, 0.0230, 0.0399, 0.0681, 0.0430, 0.0191, 0.0456,\n",
      "        0.2365, 0.3555, 0.0556, 9.0396]), 'bond': tensor([0.0000, 0.1958, 0.0242, 0.0442, 0.0701, 0.0459, 0.0208, 0.8152, 0.6660]), 'global': tensor([2.8185, 2.8375, 4.6976])}\n",
      "... > feature std(s):  \n",
      " {'atom': tensor([0.3915, 0.4605, 0.3108, 0.1242, 0.1615, 0.2064, 0.1671, 0.1135, 0.1718,\n",
      "        0.3286, 0.3465, 0.1882, 5.6699]), 'bond': tensor([0.0010, 0.3121, 0.1273, 0.1693, 0.2090, 0.1723, 0.1183, 0.0937, 0.1894]), 'global': tensor([0.1632, 0.1720, 0.0635])}\n",
      "... > Scaling targets\n",
      "mean [0.21248137]\n",
      "std [0.04254279]\n",
      "... > Scaling targets complete\n",
      "... > feature mean(s): \n",
      " {'global': tensor([0.2125])}\n",
      "... > feature std(s):  \n",
      " {'global': tensor([0.0425])}\n",
      "... > loaded dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = HeteroGraphGraphLabelDataset(\n",
    "    file=config[\"dataset\"][\"train_dataset_loc\"],\n",
    "    allowed_ring_size=config[\"dataset\"][\"allowed_ring_size\"],\n",
    "    allowed_charges=config[\"dataset\"][\"allowed_charges\"],\n",
    "    self_loop=True,\n",
    "    extra_keys=config[\"dataset\"][\"extra_keys\"],\n",
    "    target_list=config[\"dataset\"][\"target_list\"],\n",
    "    extra_dataset_info=config[\"dataset\"][\"extra_dataset_info\"],\n",
    "    debug=config[\"debug\"],\n",
    "    standard_scale_features=config[\"standard_scale_features\"],\n",
    "    standard_scale_targets=config[\"standard_scale_targets\"],\n",
    "    log_scale_features=config[\"log_scale_features\"],\n",
    "    log_scale_targets=config[\"log_scale_targets\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > creating MoleculeWrapper objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 11429.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > bond_feats_error_count:  0\n",
      "... > atom_feats_error_count:  0\n",
      "element set {'N', 'C', 'H', 'O'}\n",
      "selected atomic keys ['extra_feat_atom_esp_total']\n",
      "selected bond keys ['extra_feat_bond_esp_total', 'bond_length']\n",
      "selected global keys ['extra_feat_global_E1_CAM']\n",
      "... > Building graphs and featurizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 233.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included in labels\n",
      "{'global': ['extra_feat_global_E1_CAM']}\n",
      "included in graph features\n",
      "{'atom': ['total_degree', 'total_H', 'is_in_ring', 'ring_size_3', 'ring_size_4', 'ring_size_5', 'ring_size_6', 'ring_size_7', 'chemical_symbol_N', 'chemical_symbol_C', 'chemical_symbol_H', 'chemical_symbol_O', 'extra_feat_atom_esp_total'], 'bond': ['metal bond', 'ring inclusion', 'ring size_3', 'ring size_4', 'ring size_5', 'ring size_6', 'ring size_7', 'bond_length', 'extra_feat_bond_esp_total'], 'global': ['num atoms', 'num bonds', 'molecule weight']}\n",
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys([])\n",
      "include names:  dict_keys(['global'])\n",
      "... > parsing labels and features in graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 35151.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys(['global'])\n",
      "... > Scaling features\n",
      "mean [2.04383219e+00 5.20350657e-01 2.78647464e-01 3.31872260e-02\n",
      " 5.76080150e-02 9.83093300e-02 6.19912336e-02 2.75516594e-02\n",
      " 6.57482780e-02 3.41264872e-01 5.12836569e-01 8.01502818e-02\n",
      " 2.08447046e+06]\n",
      "std [1.25340657e+00 8.73526527e-01 4.48333642e-01 1.79125191e-01\n",
      " 2.33000712e-01 2.97732440e-01 2.41139629e-01 1.63684347e-01\n",
      " 2.47841566e-01 4.74134115e-01 4.99835195e-01 2.71525715e-01\n",
      " 5.20377426e+06]\n",
      "mean [0.         0.28247549 0.03492647 0.06372549 0.10110294 0.06617647\n",
      " 0.03002451 1.26981778 0.98419271]\n",
      "std [0.         0.45020338 0.18359361 0.24426328 0.30146498 0.24859032\n",
      " 0.17065474 0.22397461 0.42073796]\n",
      "Standard deviation for feature 0 is 0.0, smaller than 0.001. You may want to exclude this feature.\n",
      "mean [ 15.97        16.32       108.90153938]\n",
      "std [2.67003745 2.85615126 6.59715147]\n",
      "... > Scaling features complete\n",
      "... > feature mean(s): \n",
      " {'atom': tensor([2.0438e+00, 5.2035e-01, 2.7865e-01, 3.3187e-02, 5.7608e-02, 9.8309e-02,\n",
      "        6.1991e-02, 2.7552e-02, 6.5748e-02, 3.4126e-01, 5.1284e-01, 8.0150e-02,\n",
      "        2.0845e+06]), 'bond': tensor([0.0000, 0.2825, 0.0349, 0.0637, 0.1011, 0.0662, 0.0300, 1.2698, 0.9842]), 'global': tensor([ 15.9700,  16.3200, 108.9015])}\n",
      "... > feature std(s):  \n",
      " {'atom': tensor([1.2534e+00, 8.7353e-01, 4.4833e-01, 1.7913e-01, 2.3300e-01, 2.9773e-01,\n",
      "        2.4114e-01, 1.6368e-01, 2.4784e-01, 4.7413e-01, 4.9984e-01, 2.7153e-01,\n",
      "        5.2038e+06]), 'bond': tensor([0.0010, 0.4502, 0.1836, 0.2443, 0.3015, 0.2486, 0.1707, 0.2240, 0.4207]), 'global': tensor([2.6700, 2.8562, 6.5972])}\n",
      "... > Scaling targets\n",
      "mean [0.21248137]\n",
      "std [0.04254279]\n",
      "... > Scaling targets complete\n",
      "... > feature mean(s): \n",
      " {'global': tensor([0.2125])}\n",
      "... > feature std(s):  \n",
      " {'global': tensor([0.0425])}\n",
      "... > loaded dataset\n",
      "training set size:  75\n",
      "validation set size:  15\n",
      "test set size:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from qtaim_embed.core.datamodule import QTAIMGraphTaskDataModule\n",
    "dm = QTAIMGraphTaskDataModule(config=config,)\n",
    "feat_names, feat_size = dm.prepare_data(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readout in size 50\n",
      "... > number of tasks: 1\n"
     ]
    }
   ],
   "source": [
    "model = GCNGraphPred(\n",
    "    atom_input_size=feat_size[\"atom\"],\n",
    "    bond_input_size=feat_size[\"bond\"],\n",
    "    global_input_size=feat_size[\"global\"],\n",
    "    n_conv_layers=4,\n",
    "    resid_n_graph_convs=2,\n",
    "    target_dict={\"global\": [\"extra_feat_global_E1_CAM\"]},\n",
    "    conv_fn=\"GATConv\",\n",
    "    #conv_fn = \"GraphConvDropoutBatch\",\n",
    "    global_pooling=\"Set2SetThenCat\",\n",
    "    dropout=0.2,\n",
    "    batch_norm=False,\n",
    "    activation=\"ReLU\",\n",
    "    bias=True,\n",
    "    norm=\"both\",\n",
    "    aggregate=\"sum\",\n",
    "    lr=0.01,\n",
    "    scheduler_name=\"reduce_on_plateau\",\n",
    "    weight_decay=0.00001,\n",
    "    lr_plateau_patience=25,\n",
    "    lr_scale_factor=0.8,\n",
    "    loss_fn=\"mae\",\n",
    "    embedding_size=8,\n",
    "    fc_layer_size=[256, 128, 128],\n",
    "    fc_dropout=0.2,\n",
    "    fc_batch_norm=True,\n",
    "    lstm_iters=3,\n",
    "    lstm_layers=2,\n",
    "    num_heads_gat=3,\n",
    "    residual_gat=False,\n",
    "    dropout_feat_gat=0.2,\n",
    "    dropout_attn_gat=0.2,\n",
    "    hidden_size_gat=10,\n",
    "    pooling_ntypes=[\"atom\", \"bond\", \"global\"],\n",
    "    pooling_ntypes_direct=[\"global\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 1/1 [00:00<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.467737713409779 1.2209205627441406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 1/1 [00:00<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.5103242613391865 0.964249849319458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 1/1 [00:00<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.209045584721519 1.3612394332885742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 1/1 [00:00<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.904939138176479 1.191694736480713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 1/1 [00:00<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9826188488164207 1.3223180770874023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 1/1 [00:00<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.883916969396379 1.3778098821640015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|| 1/1 [00:00<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.667436912995928 1.234472393989563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|| 1/1 [00:00<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.188483220648719 0.9202025532722473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 1/1 [00:00<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.823995437666293 1.071344256401062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|| 1/1 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.586437587571186 1.0497905015945435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|| 1/1 [00:00<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.57487310814172 0.9825983643531799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|| 1/1 [00:00<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.539235200384843 1.1212078332901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|| 1/1 [00:00<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.162056636863946 1.100432276725769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|| 1/1 [00:00<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.210675097394636 0.8655080199241638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|| 1/1 [00:00<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.47215696519702 0.997499406337738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|| 1/1 [00:00<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.407340367611664 1.0115530490875244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|| 1/1 [00:00<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.763006332084721 1.039373517036438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|| 1/1 [00:00<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.738207818948376 0.9041998386383057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|| 1/1 [00:00<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.947311490162636 1.1738860607147217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|| 1/1 [00:00<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.394736752898765 1.0150972604751587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|| 1/1 [00:00<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.14182036614903 0.9066973924636841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|| 1/1 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.582044993508287 0.8875853419303894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|| 1/1 [00:00<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.005941347403226 0.9651028513908386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|| 1/1 [00:00<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.078358386186386 1.0156255960464478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|| 1/1 [00:00<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.1629527759813865 0.9143194556236267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|| 1/1 [00:00<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.07093750332809 1.0094817876815796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|| 1/1 [00:00<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.6988895924297354 0.8404616117477417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|| 1/1 [00:00<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7376389957737404 0.9762961864471436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|| 1/1 [00:00<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6528682910581614 0.8013170957565308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|| 1/1 [00:00<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.4542383582026353 0.8800631761550903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|| 1/1 [00:00<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3015876189690285 0.7143458724021912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m tq\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m training_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m step, (batch_graph, batch_label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tq):\n\u001b[1;32m     24\u001b[0m     \u001b[39m# forward propagation by using all nodes and extracting the user embeddings\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     batch_graph, batch_label \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloader))\n\u001b[1;32m     26\u001b[0m     labels \u001b[39m=\u001b[39m batch_label[\u001b[39m\"\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# basic training loop\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "import tqdm.notebook as tq\n",
    "import numpy as np\n",
    "\n",
    "# move model to cpu\n",
    "model = model.cpu()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "dataloader = dm.train_dataloader()\n",
    "\n",
    "for epoch in range(50):\n",
    "    training_loss_list = []\n",
    "    with tqdm(dataloader) as tq:\n",
    "        model.train()\n",
    "        r2_list = []\n",
    "        tq.set_description(f\"Epoch {epoch+1}\")\n",
    "        training_loss = 0\n",
    "        for step, (batch_graph, batch_label) in enumerate(tq):\n",
    "            # forward propagation by using all nodes and extracting the user embeddings\n",
    "            batch_graph, batch_label = next(iter(dataloader))\n",
    "            labels = batch_label[\"global\"]\n",
    "            logits = model.forward(batch_graph, batch_graph.ndata[\"feat\"])\n",
    "            loss = F.mse_loss(logits, labels)\n",
    "            training_loss_list.append(loss.item())\n",
    "            # loss_mae = F.l1_loss(logits, labels)\n",
    "            # compute r2 score\n",
    "            r2 = r2_score(logits.detach().numpy(), labels.detach().numpy())\n",
    "            r2_list.append(r2)\n",
    "            # Compute validation accuracy.  Omitted in this example.\n",
    "            # backward propagation\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            training_loss += loss.item()\n",
    "            # tq.set_postfix({\"Step\": step, \"MSE\": loss.item()})\n",
    "\n",
    "        r2_mean = np.mean(r2_list)\n",
    "        loss = np.mean(training_loss_list)\n",
    "        tq.set_postfix({\"final_t_loss\": training_loss, \"R_2\": r2_mean})\n",
    "        print(r2_mean, loss)\n",
    "\n",
    "        # tq.update()\n",
    "        tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom qtaim_embed.models.utils import LogParameters\\nimport wandb\\n\\nwith wandb.init(project=\"qtaim_embed_test\") as run:\\n    logger_tb = TensorBoardLogger(\"./test_logs\", name=\"test_logs\")\\n    torch.set_float32_matmul_precision(\"high\")\\n\\n    checkpoint_callback = ModelCheckpoint(\\n        dirpath=\"test_logs\",\\n        filename=\"model_lightning_{epoch:02d}-{val_mae:.2f}\",\\n        monitor=\"val_mae\",\\n        mode=\"min\",\\n        auto_insert_metric_name=True,\\n        save_last=True,\\n    )\\n\\n    early_stopping_callback = EarlyStopping(\\n        monitor=\"val_mae\",\\n        min_delta=0.00,\\n        patience=500,\\n        verbose=False,\\n        mode=\"min\",\\n    )\\n    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\\n    logger_wb = WandbLogger(name=\"test_logs\")\\n    log_parameters = LogParameters()\\n    trainer_transfer = pl.Trainer(\\n        max_epochs=100,\\n        accelerator=\"gpu\",\\n        devices=1,\\n        enable_progress_bar=True,\\n        gradient_clip_val=3.0,\\n        default_root_dir=\"./test/\",\\n        precision=\"32\",\\n        log_every_n_steps=10,\\n        callbacks=[\\n            early_stopping_callback,\\n            lr_monitor,\\n            log_parameters,\\n            checkpoint_callback,\\n        ],\\n        enable_checkpointing=True,\\n        logger=[logger_tb, logger_wb],\\n    )\\n\\n    # move model to gpu\\n    # model = model.cuda()\\n\\n    trainer_transfer.fit(model, dm)'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test migration to pytorch lightning\n",
    "\"\"\"\n",
    "import torch\n",
    "from qtaim_embed.models.utils import LogParameters\n",
    "import wandb\n",
    "\n",
    "with wandb.init(project=\"qtaim_embed_test\") as run:\n",
    "    logger_tb = TensorBoardLogger(\"./test_logs\", name=\"test_logs\")\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"test_logs\",\n",
    "        filename=\"model_lightning_{epoch:02d}-{val_mae:.2f}\",\n",
    "        monitor=\"val_mae\",\n",
    "        mode=\"min\",\n",
    "        auto_insert_metric_name=True,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_mae\",\n",
    "        min_delta=0.00,\n",
    "        patience=500,\n",
    "        verbose=False,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    logger_wb = WandbLogger(name=\"test_logs\")\n",
    "    log_parameters = LogParameters()\n",
    "    trainer_transfer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_val=3.0,\n",
    "        default_root_dir=\"./test/\",\n",
    "        precision=\"32\",\n",
    "        log_every_n_steps=10,\n",
    "        callbacks=[\n",
    "            early_stopping_callback,\n",
    "            lr_monitor,\n",
    "            log_parameters,\n",
    "            checkpoint_callback,\n",
    "        ],\n",
    "        enable_checkpointing=True,\n",
    "        logger=[logger_tb, logger_wb],\n",
    "    )\n",
    "\n",
    "    # move model to gpu\n",
    "    # model = model.cuda()\n",
    "\n",
    "    trainer_transfer.fit(model, dm)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtaim_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
