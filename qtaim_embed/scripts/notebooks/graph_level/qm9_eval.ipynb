{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dir\n",
    "import os\n",
    "\n",
    "model_dir = \"../../save_models/qm9/1027/\"\n",
    "from qtaim_embed.models.graph_level.base_gcn import GCNGraphPred\n",
    "from qtaim_embed.models.utils import load_graph_level_model_from_config\n",
    "from qtaim_embed.utils.data import get_default_graph_level_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::RESTORING MODEL FROM EXISTING FILE:::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'activation' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['activation'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "attribute name must be string, not 'ReLU'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# model_path = \"./top_models/model_lightning_epoch=208-val_l1=1.94.ckpt\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m config[\u001b[39m\"\u001b[39m\u001b[39mrestore_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model_path\n\u001b[0;32m---> 11\u001b[0m model_restart \u001b[39m=\u001b[39m load_graph_level_model_from_config(config)\n\u001b[1;32m     12\u001b[0m \u001b[39m# load model to gpu\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model_restart\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/dev/qtaim_embed/qtaim_embed/models/utils.py:22\u001b[0m, in \u001b[0;36mload_graph_level_model_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m:::RESTORING MODEL FROM EXISTING FILE:::\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mrestore_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     model \u001b[39m=\u001b[39m GCNGraphPred\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m     23\u001b[0m         checkpoint_path\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mrestore_path\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[39m# model.to(device)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m:::MODEL LOADED:::\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1543\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1465\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1471\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1472\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1543\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1544\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1545\u001b[0m         checkpoint_path,\n\u001b[1;32m   1546\u001b[0m         map_location,\n\u001b[1;32m   1547\u001b[0m         hparams_file,\n\u001b[1;32m   1548\u001b[0m         strict,\n\u001b[1;32m   1549\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1550\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_state(\u001b[39mcls\u001b[39m, checkpoint, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[39m=\u001b[39m _load_state(\u001b[39mcls\u001b[39;49m, checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[39m=\u001b[39m checkpoint[\u001b[39m\"\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:144\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cls_spec\u001b[39m.\u001b[39mvarkw:\n\u001b[1;32m    141\u001b[0m     \u001b[39m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     _cls_kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _cls_kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 144\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_cls_kwargs)\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m    147\u001b[0m     \u001b[39m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     obj\u001b[39m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "File \u001b[0;32m~/dev/qtaim_embed/qtaim_embed/models/graph_level/base_gcn.py:158\u001b[0m, in \u001b[0;36mGCNGraphPred.__init__\u001b[0;34m(self, atom_input_size, bond_input_size, global_input_size, n_conv_layers, target_dict, conv_fn, global_pooling, resid_n_graph_convs, dropout, batch_norm, activation, bias, norm, aggregate, lr, scheduler_name, weight_decay, lr_plateau_patience, lr_scale_factor, loss_fn, embedding_size, fc_layer_size, fc_dropout, fc_batch_norm, lstm_iters, lstm_layers, output_dims, pooling_ntypes, pooling_ntypes_direct)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m# convert string activation to function\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mactivation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(torch\u001b[39m.\u001b[39;49mnn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams\u001b[39m.\u001b[39;49mactivation)()\n\u001b[1;32m    160\u001b[0m input_size \u001b[39m=\u001b[39m {\n\u001b[1;32m    161\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39matom\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39matom_input_size,\n\u001b[1;32m    162\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbond\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mbond_input_size,\n\u001b[1;32m    163\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mglobal\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mglobal_input_size,\n\u001b[1;32m    164\u001b[0m }\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(\"input size\", input_size)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: attribute name must be string, not 'ReLU'"
     ]
    }
   ],
   "source": [
    "model_list = os.listdir(model_dir)\n",
    "model_list = [os.path.join(model_dir, model) for model in model_list]\n",
    "model_list_ordered = sorted(\n",
    "    model_list, key=lambda x: int(x.split(\"=\")[-1].split(\".\")[0])\n",
    ")\n",
    "config = get_default_graph_level_config()\n",
    "for model_path in model_list_ordered:\n",
    "    config[\"restore\"] = True\n",
    "    # model_path = \"./top_models/model_lightning_epoch=208-val_l1=1.94.ckpt\"\n",
    "    config[\"restore_path\"] = model_path\n",
    "    model_restart = load_graph_level_model_from_config(config)\n",
    "    # load model to gpu\n",
    "    model_restart.cuda()\n",
    "    model_list.append(model_restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>config_settings<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "import wandb, argparse, torch, json\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from qtaim_embed.core.datamodule import QTAIMGraphTaskDataModule\n",
    "from qtaim_embed.models.utils import LogParameters, load_graph_level_model_from_config\n",
    "from qtaim_embed.utils.data import get_default_graph_level_config\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")  # might have to disable on older GPUs\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "\n",
    "on_gpu = bool(True)\n",
    "debug = bool(False)\n",
    "project_name = \"qm9_eval\"\n",
    "dataset_loc = \"../../../data/qm9_qtaim_1025_labelled.pkl\"\n",
    "log_save_dir = \"./qm9_eval/\"\n",
    "config = None\n",
    "if config is None:\n",
    "    config = get_default_graph_level_config()\n",
    "\n",
    "config[\"dataset\"][\"log_scale_features\"] = True\n",
    "config[\"dataset\"][\"standard_scale_features\"] = True\n",
    "config[\"dataset\"][\"standard_scale_targets\"] = True\n",
    "config[\"dataset\"][\"target_list\"] = [\"u0\"]\n",
    "config[\"dataset\"][\"train_batch_size\"] = 512\n",
    "config[\"dataset\"][\"extra_keys\"] = {\n",
    "    \"atom\": [\"extra_feat_atom_esp_total\"],\n",
    "    \"bond\": [\n",
    "        \"extra_feat_bond_esp_total\",\n",
    "        \"bond_length\",\n",
    "    ],\n",
    "    \"global\": [\"u0\"],\n",
    "}\n",
    "\n",
    "config[\"model\"] = {\n",
    "    \"n_conv_layers\": 4,\n",
    "    \"resid_n_graph_convs\": 1,\n",
    "    \"conv_fn\": \"GraphConvDropoutBatch\",\n",
    "    \"global_pooling_fn\": \"SumPoolingThenCat\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_norm\": True,\n",
    "    \"activation\": \"ReLU\",\n",
    "    \"bias\": True,\n",
    "    \"norm\": \"both\",\n",
    "    \"aggregate\": \"sum\",\n",
    "    \"lr\": 0.0002,\n",
    "    \"scheduler_name\": \"reduce_on_plateau\",\n",
    "    \"weight_decay\": 0.00001,\n",
    "    \"lr_plateau_patience\": 50,\n",
    "    \"lr_scale_factor\": 0.75,\n",
    "    \"loss_fn\": \"mse\",\n",
    "    \"embedding_size\": 50,\n",
    "    # \"fc_layer_size\": [256, 128],\n",
    "    \"shape_fc\": \"cone\",\n",
    "    \"fc_hidden_size_1\": 512,\n",
    "    \"fc_num_layers\": 1,\n",
    "    \"fc_dropout\": 0.2,\n",
    "    \"fc_batch_norm\": True,\n",
    "    \"lstm_iters\": 3,\n",
    "    \"lstm_layers\": 2,\n",
    "    \"output_dims\": 1,\n",
    "    \"pooling_ntypes\": [\"atom\", \"bond\", \"global\"],\n",
    "    \"pooling_ntypes_direct\": [\"global\"],\n",
    "    \"restore\": False,\n",
    "    \"max_epochs\": 10,\n",
    "}\n",
    "\n",
    "if config[\"optim\"][\"precision\"] == \"16\" or config[\"optim\"][\"precision\"] == \"32\":\n",
    "    config[\"optim\"][\"precision\"] = int(config[\"optim\"][\"precision\"])\n",
    "\n",
    "# set log save dir\n",
    "config[\"dataset\"][\"log_save_dir\"] = log_save_dir\n",
    "\n",
    "# dataset\n",
    "if dataset_loc is not None:\n",
    "    config[\"dataset\"][\"train_dataset_loc\"] = dataset_loc\n",
    "extra_keys = config[\"dataset\"][\"extra_keys\"]\n",
    "\n",
    "if debug:\n",
    "    config[\"dataset\"][\"debug\"] = debug\n",
    "print(\">\" * 40 + \"config_settings\" + \"<\" * 40)\n",
    "\n",
    "# for k, v in config.items():\n",
    "#    print(\"{}\\t\\t\\t{}\".format(str(k).ljust(20), str(v).ljust(20)))\n",
    "dm = QTAIMGraphTaskDataModule(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > creating MoleculeWrapper objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133848/133848 [00:11<00:00, 11959.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > bond_feats_error_count:  0\n",
      "... > atom_feats_error_count:  0\n",
      "element set {'O', 'F', 'H', 'C', 'N'}\n",
      "selected atomic keys ['extra_feat_atom_esp_total']\n",
      "selected bond keys ['extra_feat_bond_esp_total', 'bond_length']\n",
      "selected global keys ['u0']\n",
      "... > Building graphs and featurizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133848/133848 [03:57<00:00, 564.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "included in labels\n",
      "{'global': ['u0']}\n",
      "included in graph features\n",
      "{'atom': ['total_degree', 'total_H', 'is_in_ring', 'ring_size_3', 'ring_size_4', 'ring_size_5', 'ring_size_6', 'ring_size_7', 'chemical_symbol_O', 'chemical_symbol_F', 'chemical_symbol_H', 'chemical_symbol_C', 'chemical_symbol_N', 'extra_feat_atom_esp_total'], 'bond': ['metal bond', 'ring inclusion', 'ring size_3', 'ring size_4', 'ring size_5', 'ring size_6', 'ring size_7', 'bond_length', 'extra_feat_bond_esp_total'], 'global': ['num atoms', 'num bonds', 'molecule weight']}\n",
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys([])\n",
      "include names:  dict_keys(['global'])\n",
      "... > parsing labels and features in graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133848/133848 [00:04<00:00, 33130.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loader node types: dict_keys(['atom', 'bond', 'global'])\n",
      "original loader label types: dict_keys(['global'])\n",
      "... > Log scaling features\n",
      "... > Log scaling features complete\n",
      "... > Scaling features\n",
      "mean [7.15019050e-01 2.09609839e-01 4.42638150e-02 5.92276711e-03\n",
      " 1.63601755e-02 1.57111131e-02 5.19192391e-03 1.07783534e-03\n",
      " 5.41201195e-02 9.54300379e-04 3.54128949e-01 2.43707123e-01\n",
      " 4.02366904e-02 8.24172281e+00]\n",
      "std [0.6036383  0.37273362 0.16947582 0.06379867 0.10522525 0.10316625\n",
      " 0.05976463 0.02731184 0.18596833 0.02570136 0.34649123 0.3309558\n",
      " 0.16208318 5.5485659 ]\n",
      "mean [0.         0.05763829 0.00770921 0.02136993 0.02054987 0.00689768\n",
      " 0.00150661 0.89991691 0.68164747]\n",
      "std [0.         0.19138873 0.07269242 0.11981584 0.11756609 0.06880064\n",
      " 0.0322805  0.37276119 0.22762408]\n",
      "Standard deviation for feature 0 is 0.0, smaller than 0.001. You may want to exclude this feature.\n",
      "mean [2.93093013 2.69683748 4.81626082]\n",
      "std [0.16152484 0.13399224 0.06767423]\n",
      "... > Scaling features complete\n",
      "... > feature mean(s): \n",
      " {'atom': tensor([7.1502e-01, 2.0961e-01, 4.4264e-02, 5.9228e-03, 1.6360e-02, 1.5711e-02,\n",
      "        5.1919e-03, 1.0778e-03, 5.4120e-02, 9.5430e-04, 3.5413e-01, 2.4371e-01,\n",
      "        4.0237e-02, 8.2417e+00]), 'bond': tensor([0.0000, 0.0576, 0.0077, 0.0214, 0.0205, 0.0069, 0.0015, 0.8999, 0.6816]), 'global': tensor([2.9309, 2.6968, 4.8163])}\n",
      "... > feature std(s):  \n",
      " {'atom': tensor([0.6036, 0.3727, 0.1695, 0.0638, 0.1052, 0.1032, 0.0598, 0.0273, 0.1860,\n",
      "        0.0257, 0.3465, 0.3310, 0.1621, 5.5486]), 'bond': tensor([0.0010, 0.1914, 0.0727, 0.1198, 0.1176, 0.0688, 0.0323, 0.3728, 0.2276]), 'global': tensor([0.1615, 0.1340, 0.0677])}\n",
      "... > Scaling targets\n",
      "mean [-411.54567763]\n",
      "std [40.05977789]\n",
      "... > Scaling targets complete\n",
      "... > feature mean(s): \n",
      " {'global': tensor([-411.5457])}\n",
      "... > feature std(s):  \n",
      " {'global': tensor([40.0598])}\n",
      "... > loaded dataset\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>config_settings<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "dataset             \t\t\t{'allowed_ring_size': [3, 4, 5, 6, 7], 'allowed_charges': None, 'self_loop': True, 'extra_keys': {'atom': ['extra_feat_atom_esp_total'], 'bond': ['extra_feat_bond_esp_total', 'bond_length'], 'global': ['u0']}, 'target_list': ['u0'], 'extra_dataset_info': {}, 'debug': False, 'log_scale_features': True, 'log_scale_targets': False, 'standard_scale_features': True, 'standard_scale_targets': True, 'val_prop': 0.15, 'test_prop': 0.1, 'seed': 42, 'train_batch_size': 512, 'test_dataset_loc': None, 'train_dataset_loc': '../../../data/qm9_qtaim_1025_labelled.pkl', 'num_workers': 1, 'log_save_dir': './qm9_eval/'}\n",
      "model               \t\t\t{'n_conv_layers': 4, 'resid_n_graph_convs': 1, 'conv_fn': 'GraphConvDropoutBatch', 'global_pooling_fn': 'SumPoolingThenCat', 'dropout': 0.2, 'batch_norm': True, 'activation': 'ReLU', 'bias': True, 'norm': 'both', 'aggregate': 'sum', 'lr': 0.0002, 'scheduler_name': 'reduce_on_plateau', 'weight_decay': 1e-05, 'lr_plateau_patience': 50, 'lr_scale_factor': 0.75, 'loss_fn': 'mse', 'embedding_size': 50, 'shape_fc': 'cone', 'fc_hidden_size_1': 512, 'fc_num_layers': 1, 'fc_dropout': 0.2, 'fc_batch_norm': True, 'lstm_iters': 3, 'lstm_layers': 2, 'output_dims': 1, 'pooling_ntypes': ['atom', 'bond', 'global'], 'pooling_ntypes_direct': ['global'], 'restore': False, 'max_epochs': 10, 'atom_feature_size': 14, 'bond_feature_size': 9, 'global_feature_size': 3, 'target_dict': {'global': ['u0']}}\n",
      "optim               \t\t\t{'num_devices': 1, 'num_nodes': 1, 'gradient_clip_val': 5.0, 'strategy': 'auto', 'precision': 'bf16', 'accumulate_grad_batches': 3}\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>config_settings<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model constructed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanti\u001b[0m (\u001b[33mhydro_homies\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/santiagovargas/dev/qtaim_embed/qtaim_embed/scripts/notebooks/wandb/run-20231027_113101-73uvbee9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hydro_homies/qm9_eval/runs/73uvbee9' target=\"_blank\">curious-elevator-2</a></strong> to <a href='https://wandb.ai/hydro_homies/qm9_eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hydro_homies/qm9_eval' target=\"_blank\">https://wandb.ai/hydro_homies/qm9_eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hydro_homies/qm9_eval/runs/73uvbee9' target=\"_blank\">https://wandb.ai/hydro_homies/qm9_eval/runs/73uvbee9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:398: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/lightning_fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/santiagovargas/dev/qtaim_embed/qtaim_embed/scripts/notebooks/qm9_eval exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name            | Type               | Params\n",
      "--------------------------------------------------------\n",
      "0  | embedding       | UnifySize          | 1.3 K \n",
      "1  | conv_layers     | ModuleList         | 95.4 K\n",
      "2  | readout         | SumPoolingThenCat  | 0     \n",
      "3  | loss            | MultioutputWrapper | 0     \n",
      "4  | fc_layers       | ModuleList         | 78.8 K\n",
      "5  | train_r2        | MultioutputWrapper | 0     \n",
      "6  | train_torch_l1  | MultioutputWrapper | 0     \n",
      "7  | train_torch_mse | MultioutputWrapper | 0     \n",
      "8  | val_r2          | MultioutputWrapper | 0     \n",
      "9  | val_torch_l1    | MultioutputWrapper | 0     \n",
      "10 | val_torch_mse   | MultioutputWrapper | 0     \n",
      "11 | test_r2         | MultioutputWrapper | 0     \n",
      "12 | test_torch_l1   | MultioutputWrapper | 0     \n",
      "13 | test_torch_mse  | MultioutputWrapper | 0     \n",
      "--------------------------------------------------------\n",
      "175 K     Trainable params\n",
      "0         Non-trainable params\n",
      "175 K     Total params\n",
      "0.702     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b21ac7a25b845ca88f8873b2f499003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9794f814e24d4181c9e7d403e75bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0868ff28bef422089d03a3bc48dfe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('val_mae', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8924f56b878401b8f97f053340b9b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06940f0bd6a74541b37b318ba630ee40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8e654cecd54ef693168b157b427731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd9559a00d9499e96460bac36618eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78bf6da716d49508b9865af5e5b147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b1d6955dbc4da7bacc51fd442667d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f3fd9d3c524d4d8437e4cea3bb5f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815d19d162d347678df4a97722162918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c8ba098c5743ac85788e59636b5692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/santiagovargas/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0cae8953be48d58b57521f3f033fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Runningstage.testing    </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.005003075581043959    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.04592243209481239    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.07073242217302322    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9949820637702942     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.005003075581043959   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04592243209481239   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.07073242217302322   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9949820637702942    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2810b4768840c1a941ed3ea19fdf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.226765…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_mae</td><td>▁</td></tr><tr><td>test_mse</td><td>▁</td></tr><tr><td>test_r2</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train_mae</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_mse</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_r2</td><td>▁▆▇▇▇█████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr><tr><td>val_mae</td><td>▁█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▅▄▃▂▂▂▂▁▁</td></tr><tr><td>val_r2</td><td>▁▄▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>lr-Adam</td><td>0.0002</td></tr><tr><td>test_loss</td><td>0.005</td></tr><tr><td>test_mae</td><td>0.04592</td></tr><tr><td>test_mse</td><td>0.07073</td></tr><tr><td>test_r2</td><td>0.99498</td></tr><tr><td>train_loss</td><td>0.03301</td></tr><tr><td>train_mae</td><td>0.12893</td></tr><tr><td>train_mse</td><td>0.18169</td></tr><tr><td>train_r2</td><td>0.96699</td></tr><tr><td>trainer/global_step</td><td>660</td></tr><tr><td>val_loss</td><td>0.00505</td></tr><tr><td>val_mae</td><td>0.04585</td></tr><tr><td>val_mse</td><td>0.07103</td></tr><tr><td>val_r2</td><td>0.99497</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curious-elevator-2</strong> at: <a href='https://wandb.ai/hydro_homies/qm9_eval/runs/73uvbee9' target=\"_blank\">https://wandb.ai/hydro_homies/qm9_eval/runs/73uvbee9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231027_113101-73uvbee9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_names, feature_size = dm.prepare_data(stage=\"fit\")\n",
    "config[\"model\"][\"atom_feature_size\"] = feature_size[\"atom\"]\n",
    "config[\"model\"][\"bond_feature_size\"] = feature_size[\"bond\"]\n",
    "config[\"model\"][\"global_feature_size\"] = feature_size[\"global\"]\n",
    "config[\"model\"][\"target_dict\"] = {}\n",
    "config[\"model\"][\"target_dict\"][\"global\"] = config[\"dataset\"][\"target_list\"]\n",
    "# config[\"dataset\"][\"feature_names\"] = feature_names\n",
    "\n",
    "print(\">\" * 40 + \"config_settings\" + \"<\" * 40)\n",
    "for k, v in config.items():\n",
    "    print(\"{}\\t\\t\\t{}\".format(str(k).ljust(20), str(v).ljust(20)))\n",
    "\n",
    "print(\">\" * 40 + \"config_settings\" + \"<\" * 40)\n",
    "\n",
    "model = load_graph_level_model_from_config(config[\"model\"])\n",
    "print(\"model constructed!\")\n",
    "\n",
    "with wandb.init(project=project_name) as run:\n",
    "    log_parameters = LogParameters()\n",
    "    logger_tb = TensorBoardLogger(config[\"dataset\"][\"log_save_dir\"], name=\"test_logs\")\n",
    "    logger_wb = WandbLogger(project=project_name, name=\"test_logs\")\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=config[\"dataset\"][\"log_save_dir\"],\n",
    "        filename=\"model_lightning_{epoch:02d}-{val_mae:.2f}\",\n",
    "        monitor=\"val_mae\",\n",
    "        mode=\"min\",\n",
    "        auto_insert_metric_name=True,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor=\"val_mae\", min_delta=0.00, patience=200, verbose=False, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config[\"model\"][\"max_epochs\"],\n",
    "        accelerator=\"gpu\",\n",
    "        devices=config[\"optim\"][\"num_devices\"],\n",
    "        num_nodes=config[\"optim\"][\"num_nodes\"],\n",
    "        gradient_clip_val=config[\"optim\"][\"gradient_clip_val\"],\n",
    "        accumulate_grad_batches=config[\"optim\"][\"accumulate_grad_batches\"],\n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[\n",
    "            early_stopping_callback,\n",
    "            lr_monitor,\n",
    "            log_parameters,\n",
    "            checkpoint_callback,\n",
    "        ],\n",
    "        enable_checkpointing=True,\n",
    "        strategy=config[\"optim\"][\"strategy\"],\n",
    "        default_root_dir=config[\"dataset\"][\"log_save_dir\"],\n",
    "        logger=[logger_tb, logger_wb],\n",
    "        precision=config[\"optim\"][\"precision\"],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, dm)\n",
    "    trainer.test(model, dm)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  2\n",
      "r2:  0.9751056\n",
      "mae:  4.5093374\n",
      "mse:  6.3279834\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "dl_test = dm.val_dataloader()\n",
    "r2_list, mae_list, mse_list = [], [], []\n",
    "# iterate over batches\n",
    "for batch in dl_test:\n",
    "    print(\"batch: \", len(batch))\n",
    "    # batch_graphs\n",
    "    batch_graphs = batch[0]\n",
    "    \n",
    "    batch_label = batch_graphs.ndata[\"labels\"]\n",
    "    scaler_list = dm.test_dataset.dataset.label_scalers\n",
    "    r2_val, mae_val, mse_val = model.evaluate_manually(\n",
    "        batch_graphs, batch_label, scaler_list\n",
    "    )\n",
    "    r2_list.append(r2_val)\n",
    "    mae_list.append(mae_val)\n",
    "    mse_list.append(mse_val)\n",
    "\n",
    "print(\"r2: \", np.mean(r2_list))\n",
    "print(\"mae: \", np.mean(mae_list))\n",
    "print(\"mse: \", np.mean(mse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4.5498)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import QM9Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/santiagovargas/.dgl/qm9_eV.npz from https://data.dgl.ai/dataset/qm9_eV.npz...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'u0 is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m QM9Dataset(label_keys\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mu0\u001b[39;49m\u001b[39m\"\u001b[39;49m],transform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/data/qm9.py:122\u001b[0m, in \u001b[0;36mQM9Dataset.__init__\u001b[0;34m(self, label_keys, cutoff, raw_dir, force_reload, verbose, transform)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_keys \u001b[39m=\u001b[39m label_keys\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url \u001b[39m=\u001b[39m _get_dgl_url(\u001b[39m\"\u001b[39m\u001b[39mdataset/qm9_eV.npz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m \u001b[39msuper\u001b[39;49m(QM9Dataset, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    123\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mqm9\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    124\u001b[0m     url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_url,\n\u001b[1;32m    125\u001b[0m     raw_dir\u001b[39m=\u001b[39;49mraw_dir,\n\u001b[1;32m    126\u001b[0m     force_reload\u001b[39m=\u001b[39;49mforce_reload,\n\u001b[1;32m    127\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    128\u001b[0m     transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/data/dgl_dataset.py:112\u001b[0m, in \u001b[0;36mDGLDataset.__init__\u001b[0;34m(self, name, url, raw_dir, save_dir, hash_key, force_reload, verbose, transform)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_dir \u001b[39m=\u001b[39m save_dir\n\u001b[0;32m--> 112\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/data/dgl_dataset.py:203\u001b[0m, in \u001b[0;36mDGLDataset._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m load_flag:\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/data/qm9.py:141\u001b[0m, in \u001b[0;36mQM9Dataset.process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR \u001b[39m=\u001b[39m data_dict[\u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mZ \u001b[39m=\u001b[39m data_dict[\u001b[39m\"\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(\n\u001b[0;32m--> 141\u001b[0m     [data_dict[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_keys], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_cumsum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([[\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mcumsum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN)])\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/dgl/data/qm9.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mR \u001b[39m=\u001b[39m data_dict[\u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mZ \u001b[39m=\u001b[39m data_dict[\u001b[39m\"\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(\n\u001b[0;32m--> 141\u001b[0m     [data_dict[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_keys], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN_cumsum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([[\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mcumsum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN)])\n",
      "File \u001b[0;32m~/anaconda3/envs/qtaim_embed/lib/python3.11/site-packages/numpy/lib/npyio.py:260\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n\u001b[1;32m    259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not a file in the archive\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'u0 is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "QM9Dataset(label_keys=[\"u0\"],transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtaim_embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
